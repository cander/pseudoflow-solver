\section{Pseudoflow Solver}
The solver manages the entire problem instance.  Initially
this includes the nodes and edges.  These are stored as contiguous
arrays (as opposed to allocating each one individually with [[new]])
to make memory management fast and simple.  These arrays are allocated
when the problem instance is read.
<<Solver protected members>>=
    Node*	nodes;
    Edge*	edges;
@ %def nodes edges
<<default Solver constructor>>=
    nodes = nil;
    edges = nil;
@ 
Just for safety checking, we should also keep track of the
number of elements in each of these arrays.
<<Solver protected members>>=
    int		numNodes;
    int		numEdges;
<<default Solver constructor>>=
    numNodes = numEdges = 0;
@ %def numNodes numEdges
We also need to know the source and sink nodes in the graph.
<<Solver data>>=
    NodePtr 	sourceNode;
    NodePtr 	sinkNode;
<<default Solver constructor>>=
    sourceNode = sinkNode = nil;
@ %def sourceNode sinkNode

At the highest level, solving is pretty simple: we select the
lowest labeled strong branch, process it looking for mergers and 
relabling nodes that do not merge, and continue until we cannot
find a strong branch.  We can select different policies for managing the list of strong
branches (the buckets).  

The solver function below performs the usual, full normalization 
after each merger: push the flow all the way from the strong root
to the weak root.  Later we will introduce another solver that
only performs a partial normalization after each merger.
<<Solver methods>>=
    virtual void solve(AddBranchPtr addFunc);
<<Solver method implementations>>=
void PhaseSolver::solve(AddBranchPtr addFunc)
{
    IFTRACE(int currentPhase = 0;)
    addBranchFunc = addFunc;
    renormalizeFunc = &PhaseSolver::fullRenormalize;
    initGlobalRelabel();

    NodePtr strongBranch = getLowestBranch();
    while (strongBranch != nil) {
	NodeLabel strongLabel =  strongBranch->getLabel();
	IFTRACE(
	    if ( strongLabel != currentPhase) {
		currentPhase = strongLabel;
		TRACE(trout << "Phase " << currentPhase << endl;);
	    }
	)
	<<terminate early based on counting labels>>
	Boolean performedMerger = processBranch(*strongBranch);
	STATS(performedMerger ? 0 : numEmptyBranchScans++ );
	checkForRelabel();
	strongBranch = getLowestBranch();
    }
    IFDEBUG(checkMergers());
}
@ %def solve
For each possible value for a node's label, if we track the number of 
nodes with that label, then it is possible to terminate the algorithm
early.  If the lowest labeled strong branch has label $\ell$, and
there are no nodes with label $\ell - 1$, then the algorithm is done
because the strong branch cannot merge with any weak branches, and
the labels of the strong branches will only increase further during
execution.
<<terminate early based on counting labels>>=
	if ((labelCount[strongLabel - 1] == 0) && 
	    (strongLabel > maxInitialLabel))
	{
	    Boolean earlyTerm = FALSE;
	    if (relabelFrequency > 0.0) {
		<<perform last global relabel>>
	    } else {
		earlyTerm = TRUE;
	    }

	    if (earlyTerm) {
		cout << "Early termination at label " << strongLabel << endl;
		break;
	    }
	}
<<Solver protected members>>=
    int* labelCount;
<<default Solver constructor>>=
    labelCount = nil;
@ %def labelCount 
Some of the labeling methods are fairly complex, and we have been kinda lax
@ With global relabeling, we can develop a gap in the node labels before
we are done processes - i.e. while there are still merger arcs.  If we 
perform another global relabel, that should close the gap by raising the
labels of various weak and strong nodes.  If the gap persists, we really
are done.  Note that if we are done before this final relabel, the relabel
operation should prune out all of the strong nodes because they will no
longer be able to reach the sink.  In this case, [[getLowestBranch]] should
return null.
<<perform last global relabel>>=
    addBranchFifo(*strongBranch);
cout << "Performing final global relabel at phase " << strongLabel 
     << " with " << numRemovedNodes << " nodes pruned from the graph" << endl;
    globalRelabel();
    strongBranch = getLowestBranch();
    if (strongBranch == nil) {
	break;
    }
    strongLabel =  strongBranch->getLabel();
    if ((labelCount[strongLabel - 1] == 0) && 
	(strongLabel > INIT_WEAK_LABEL))
    {
	earlyTerm = TRUE;
    }
@
\subsection{Processing Branches}
We process the tree one branch at a time looking for mergers or
relabeling nodes.  As soon as there is a merger, we stop processing
this branch to allow the main loop to fetch the next, lowest-labeled
branch from the buckets.  It could be that we resume processing this 
branch.  If there was no merger, we move the branch to the next bucket.

As an optimization, we check to see if there are any nodes that
we could possibly merge with before invesigating the branch.
If there are no nodes with a label one less than ours, we just
relabel the whole branch.  This shouldn't happen very often
any more, especially since we count the nodes with each label value
and terminate early.  It might still be invoked as a side-effect of
some initialization or normalization schemes - especially if we
queue a branch with $\ell = 1$, the initial value for weak nodes.
<<Solver methods>>=
    virtual Boolean processBranch(Node& root);
<<Solver method implementations>>=
Boolean PhaseSolver::processBranch(Node& root)
{
    Boolean performedMerger = processSubtree(root);
    if (!performedMerger) {
	addStrongBranch(root);	// XXX maybe this should be in solve
    }

    return performedMerger;
}
@ %def processBranch
Below is recursive code to perform the DFS scan of a node's children and to
look for merger arcs among its neighbors.  The code
processes the nodes in either post-order (i.e. the children are all processed
before the neighbors of this node are scanned) or pre-order
(scan the neighbors first).
If this function performs a merger, it returns immediately.
If any part of the sub-tree is split off and strong, it will
be put in the appropriate bucket where [[getLowestBranch]] will
find it.  If there is no merger, the label of this node is incremented.
The function returns true or false if we perform a merger.
<<Solver methods>>=
    Boolean processSubtree(Node& node);
<<Solver method implementations>>=
Boolean PhaseSolver::processSubtree(Node& node)
{
    NodeLabel label = node.getLabel();
    if (postOrderSearch) {
	<<process all children>>
	<<look for merger among neighbors>>
    } else {
	<<look for merger among neighbors>>
	<<process all children>>
    }

    // no merger
    <<verify no merger available>>
    <<increase node label>>
    TRACE(trout << "node " << node.getId() << " relabled to "
		<< node.getLabel() << endl;);
    return FALSE;
}
@ %def processSubtree
<<public Solver data>>=
    Boolean postOrderSearch;
<<default Solver constructor>>=
    postOrderSearch = TRUE;
@
We process all children that have the same label as ours, [[label]].
If a child has a higher label, we just ignore it.  By monotonicity,
it can't have a label less than ours.  If we process the sub-tree
and that generates a merger we return immediately.
<<process all children>>=
    while (node.hasMoreChildren()) {
	Node& child = node.getCurrentChild();
	assert(child.getLabel() >= label);
	if (node.minChildLabel > child.getLabel()) {
	    node.minChildLabel = child.getLabel();
	}
	if (child.getLabel() == label) {
	    Boolean performedMerger = processSubtree(child);
	    if (performedMerger) {
		return TRUE;
	    }
	}
	node.advanceChildren();
    }
@
Processing neighbors is a bit different.  Here is where we look for
a merger to a node with label exactly one less that ours
(given that the arc has residual capacity).  
If we find that, we will merge and return true immediately.

{\em Note: there is a slight problem below.  We check for 
a positive capacity on the arc (instead of non-negative), 
but if we ever implemented a scheme
where we didn't split immediately up zero residual capacity, we 
would have zero-capacity arcs that are in the tree (i.e. the parent
arc), but we wouldn't want to use them for mergers.  This would
be a mess, but we could overcome it.  Actually, it seems pretty
easy: during phase $\ell$, both the node and it's parent have
label $\ell$, so when we check the label of the neighbor, we
won't try to merge.}

{\em Note deux: According to the algorithm, the only merger arcs
during phase $l$ should be to nodes with label $l-1$.  However, when we
introduced global relabeling and shortest path initialization, we found
that for some unknown reason there were weak nodes with label $l-2$ that
we were not merging to.  Therefore, for the moment at least, the test
has been changed from $label = l-1$ to $label < l$.  The existance
of these super-weak mergers actually contradicts the lowest label rule
because given two strong nodes with label $l$, one might have a neighbor
with label $l-1$ and the other with $l-2$.  We according to `lowest
label' we should process the latter first, but we might not.  Maybe this
is a bug that I should find to better understand global labeling?

Note trios: We added code to check the neighbor's distance label and
merged to it only if it was less than ours, but this provided only
minimal/negligible difference.  The code still must check the node labels
to tell the difference between strong and weak nodes (weak nodes still
have label less than ours).  With a genrmf graph, I only saw a differnce of
two in the number of mergers.
}
<<look for merger among neighbors>>=
    if (node.distance <= label) {
//	int dist = node.distance;
	while (node.hasMoreNeighbors()) {
	    Edge& neighborEdge = node.getCurrentNeighbor();
	    Node& neighbor = *neighborEdge.getOtherNode(&node);
	    if ((neighbor.getLabel() < label) &&
//		(neighbor.distance < dist) &&
		(neighborEdge.residCapacity(node) > 0))
	    {
		TRACE(if (neighbor.getLabel() < (label - 1)) 
		       trout << "Super weak merger from " << node.getId() 
			     << " (l=" << node.getLabel() << ") to " 
			     << neighbor.getId() 
			     << " (l=" << neighbor.getLabel() << ")" << endl; );
		merge(node, neighbor, neighborEdge);
		return TRUE;
	    }
	    node.advanceNeighbors();
	}
    }
@
If we scan a node and do not find a merger, we want to increase the
label of the node.  At the very least, we need to increment the label of 
the node by one.  However, if we are using distance labeling,
we may be able to do more.
<<increase node label>>=
    increaseNodeLabel(node);
@
This is some debug/assertion code to verify that we haven't missed
any available merger opportunites.  The algorithm claims that the
only weak nodes we can merge with should have label $l-1$.  When
we use distance labels, we've found that sometimes we miss a
merger with a node labeled $l-2$.  This checks for that.
<<verify no merger available>>=
#ifdef DEBUG
    ElIterator it = node.getNeighbors();
    for (EdgePtr edge = it.getNext(); edge != nil; edge=it.getNext()) {
	Node& neighbor = *edge->getOtherNode(&node);
	FlowAmount residCap = edge->residCapacity(node);
	if (residCap > 0 && (neighbor.isStrong(FALSE)) == FALSE) {
	    if (neighbor.getLabel() < node.getLabel()) {
		TRACE(trout << "Missed merger from " << node.getId() << " (l="
		     << node.getLabel() << ") to " << neighbor.getId()
		     << " (l=" << neighbor.getLabel() << ")" << endl);
	    }
	}
    }
#endif /*DEBUG*/
@
Back in [[processSubtree]] we had a simple method call to increment
the label on a node.  Normally, we would just call [[incrementLabel]]
on the node, but we want to track how many nodes we have with each
label so that we can terminate the algorithm early.
<<Solver methods>>=
    void incrementLabel(Node& node);
<<Solver inline implementations>>=
INLINE void PhaseSolver::incrementLabel(Node& node)
{
    setLabel(node, node.getLabel() + 1);
}
@ %def incrementLabel
Similarly, during global relabeling, we need to change the label of
a node by more than one.  We need to update the counts when we do
this.  We also reset the iterations.
<<Solver methods>>=
    void setLabel(Node& node, NodeLabel newLabel);
<<Solver inline implementations>>=
INLINE void PhaseSolver::setLabel(Node& node, NodeLabel newLabel)
{
    assert(newLabel > node.getLabel());
    labelCount[node.getLabel()]--;
    labelCount[newLabel]++;
    STATS(numRelabels++);
    STATS(if ((newLabel - node.getLabel()) > 1) numLabelSkips++; );
    node.setLabel(newLabel);
    node.resetIterations();
    node.minChildLabel = max(node.distance, newLabel); // move to resetIterations?
    STATS(if (newLabel >= numNodes)  numRemovedNodes++);
}
@ %def setLabel
When we know that a branch cannot merge with
any weak nodes, we need to increment the label
of the branch, and all of its children that have the same label.
This is really a stripped down version of [[processSubtree]].
Given the early termination code based on counting nodes and labels,
this code isn't usually needed.  It might be useful for some
tree initialization schemes that result in strong branches with
the initial weak label.
<<Solver methods>>=
    void relabelSubtree(Node& node);
<<Solver method implementations>>=
void PhaseSolver::relabelSubtree(Node& node)
{
    NodeLabel label = node.getLabel();
    node.resetIterations();
    while (node.hasMoreChildren()) {
	Node& child = node.getCurrentChild();
	assert(child.getLabel() >= label);
	if (child.getLabel() == label) {
	    relabelSubtree(child);
	}
	node.advanceChildren();
    }

    incrementLabel(node);
    TRACE(trout << "node " << node.getId() << " Relabled to "
		<< node.getLabel() << endl;);
}
@ %def relabelSubtree
\subsection{Basic Merger Functions}
The normal merger function 
rehangs the strong branch from the strong node, adds the
merger arc from $s$ to $w$, and renormalizes the tree by
pushing the excess from the old strong root towards the weak root.  
There are different possible policies for renormalizing the tree
after a merger, so we use a pointer to function as an indirection.
<<Solver methods>>=
    virtual void merge(Node& strong, Node& weak, Edge& edge);
<<Solver method implementations>>=
void PhaseSolver::merge(Node& strong, Node& weak, Edge& edge)
{
    TRACE( trout << "merge " << strong.getId() << " ("
		   << strong.getRootExcess() << ") to " 
		   << weak.getId() << " ("
		   << weak.getExcess() << ", "
		   << weak.getRootExcess() << "), cap "
		   << edge.residCapacity(strong) << ", s-label "
		   << strong.getLabel() << endl;)
    STATS(numMergers++);

    Node& strongRoot = *strong.rehang();
    weak.addChild(strong, edge);
weak.checkTree(weak.getParentEdge());
    CHECK_TREE(*weak.getRoot(), nil);

    // renormalize the tree
    (this->*renormalizeFunc)(strongRoot, weak);
}
@ %def merge
<<Solver private members>>=
    RenormalizePtr renormalizeFunc;
<<default Solver constructor>>=
    renormalizeFunc = &PhaseSolver::fullRenormalize;
@ %def renormalizeFunc
Push excess from the former strong root, [[src]] to the strong node 
and one step beyond to the weak node, [[dest]].
When this finishes, the remaining excess will be stored on
the destination node.
We save the parent pointer before the push in case there is a
split, in which case the parent pointer will be [[nil]].
<<Solver methods>>=
    void strongPush(Node& src, Node& dest);
<<Solver method implementations>>=
void PhaseSolver::strongPush(Node& src, Node& dest)
{
    NodePtr currNode = &src;
    while (currNode != &dest) {
	NodePtr parent = currNode->getParentNode();
	STATS(numPushToParent++);
	Boolean performedSplit = currNode->pushToParent();
	<<check for zero-capcity split>>

	if (performedSplit) {
	    STATS(numSplits++);
	    if (isStrongNode(*currNode)) {	
		addStrongBranch(*currNode);
	    } 
	}
	currNode = parent;
    }
}
@ %def strongPush
We allow special handling if an edge has zero residual capacity.
It could be split or left alone.
<<check for zero-capcity split>>=
    if (performedSplit == FALSE && 
	currNode->getParentCapacity() == 0 &&
	splitOnZeroCapacity) 
    {
	STATS(numSplits++);
	currNode->split();
	performedSplit = TRUE;
    }
@
We need to determine if a node is strong.  Usually, this means that
it has positive excess, but in some cases, we can choose to treat
a node with zero excess as strong.  We want to select that at
runtime for experimentation.
<<Solver methods>>=
    Boolean isStrongNode(const Node& node);
<<Solver inline implementations>>=
INLINE Boolean PhaseSolver::isStrongNode(const Node& node)
{
    FlowAmount excess =  node.getExcess();
    return ((excess > 0) || ((excess == 0) && zeroDeficitIsStrong)) ?
	    TRUE : FALSE;
}
@ %def isStrongNode
Pushing on the weak side is similar in that it uses [[pushToParent]]
to push all of the flow, but it differs in that we push all the way
to the root of the branch.
The function returns true if we perform any splits - i.e. create
any new strong branches.
<<Solver methods>>=
    Boolean weakPush(Node& src);
<<Solver method implementations>>=
Boolean PhaseSolver::weakPush(Node& src)
{
    Boolean result = FALSE;
    NodePtr currNode = &src;
    NodePtr parent = currNode->getParentNode();

    while (parent != nil) {
	STATS(numPushToParent++);
	Boolean performedSplit = currNode->pushToParent();
	currNode->clearFlag(DEFERRED);
	<<check for zero-capcity split>>

	if (performedSplit) {
	    STATS(numSplits++);
	    CHECK_TREE(*currNode, nil);
	    if (isStrongNode(*currNode)) {
		// should worry about l=1
		addStrongBranch(*currNode);
		result = TRUE;
	    } 
	}
	currNode = parent;
	parent = currNode->getParentNode();
    }

    CHECK_TREE(*currNode, nil);
    currNode->clearFlag(DEFERRED);

    <<check for weak root becoming strong>>
    return result;
}
@ %def weakPush
After we have pushed flow all the way to the weak root, we need to
check to see if the weak root is now strong.  The weak root is
pointed to by [[currNode]].  One special case we deal with here
is if the weak root has the initial weak label, we know that when 
it goes strong, there will be no one 
for it to merge with (because the weak label is the lowest possible).
Therefore, we can just relabel it here and now before adding it.
<<check for weak root becoming strong>>=
    if (isStrongNode(*currNode)) {
	addStrongBranch(*currNode);
	result = TRUE;
    }
@
In the code above, we used some flags to control some of the
behavior of the code while it's running.  We'll declare these
public so that they can be easily inspected and tweaked.
The first flag controls what happens if we push excess along an
arc that matches the residual capacity.  We could choose to
either split it or leave it alone until the next push sends
flow along the arc, which will cause an immediate split.
<<public Solver data>>=
    Boolean splitOnZeroCapacity;
@
The second flag controls how we regard zero-deficit nodes/branches during
the execution of the algorithm (they are always weak when start the
algorithm).  If we treat a zero-deficit node as weak, when the algorithm
finishes, the strong nodes will be a minimal source set.  However, this
can lead to creating weak branch with root nodes whose label is greater
than one.  This in turn makes it very difficult (impossible?) to bound 
the maximum label that weak node can take (if weak roots have label one,
no node can have a label greater than $n$).  If we treat zero-deficit
nodes as strong, weak roots will always have label one, we will never
create any additional zero-deficit nodes/branches, but we have no easy
way to find a minimal source set.
<<public Solver data>>=
    Boolean zeroDeficitIsStrong;
<<default Solver constructor>>=
    splitOnZeroCapacity = TRUE;
    zeroDeficitIsStrong = FALSE;
@ %def splitOnZeroCapacity zeroDeficitIsStrong
After we have attached the strong branch to the weak node, we need
to renormalize the tree to push excess from the old strong root
up to the weak node and onto the weak root.  This is the default,
full normalization procedure.
<<Solver methods>>=
    void fullRenormalize(Node& strongRoot, Node& weakNode);
<<Solver method implementations>>=
void PhaseSolver::fullRenormalize(Node& strongRoot, Node& weakNode)
{
    strongPush(strongRoot, weakNode);
    weakPush(weakNode);
}
@ %def fullRenormalize
\subsection{Strong Branch Management}
To clean up the management of the buckets, let's introduce a 
simple class.

{\em Note: using a bucket class instead of separate head and
tail pointers seemed to case a performance hit of about
1.7\% - cher.20.20 w/o buckets = 35.7, with buckets =36.3 seconds. }
<<Node Bucket data>>=
private:
    NodePtr	head;
    NodePtr	tail;
<<Node Bucket methods>>=
public:
    void 	insertHead(Node& root);
    void	insertTail(Node& root);
    NodePtr	removeHead();
@
We will always implement these as inline functions:
<<Node Bucket inline implementations>>=
inline void 
NodeBucket::insertHead(Node& root)
{
    if (head == nil) {
	root.setNextNil();
	head = &root;
	tail = &root;
    } else {
	root.setNext(head);
	head = &root;
    }
}
inline void 
NodeBucket::insertTail(Node& root)
{
    if (head == nil) {
	root.setNextNil();
	head = &root;
	tail = &root;
    } else {
	tail->setNext(&root);
	tail = &root;
	root.setNextNil();
    }
}
inline NodePtr
NodeBucket::removeHead()
{
    NodePtr result = nil;
    if (head != nil) {
	result = head;
	head = result->getNext();
	if (head == nil) {
	    tail = nil;
	}
    }

    return result;
}
@ %def insertHead insertTail removeHead
Here are a couple of obvious accessor functions.
<<Node Bucket methods>>=
    NodePtr getHead() const { return head; }
    NodePtr getTail() const { return tail; }
@ %def getHead getTail
Empty a bucket.
<<Node Bucket methods>>=
    void emptyBucket() { head = tail = nil; }
@ %def emptyBucket
Typically, we remove nodes only from the head of the bucket.  However,
during global relabeling, we may wish to remove an arbitrary node.
Fortunately, this doesn't happen often, so we can affort the cost of
an $O(n)$ remove operation.
<<Node Bucket methods>>=
    void	removeNode(Node& node);
<<Solver method implementations>>=
void NodeBucket::removeNode(Node& node)
{
    NodePtr prev = nil;
    for (NodePtr curr = head; curr != nil; curr = curr->getNext()) {
	if (curr == &node) {
	    if (prev == nil) {
		removeHead();
	    } else {
		prev->setNext(curr->getNext());
		if (tail == curr) {
		    tail = prev;
		}
	    }
	    curr->setNextNil();
	    return;
	}
	prev = curr;
    }
    assert("couldn't find node in bucket" == nil);
}
<<Solver private members>>=
    NodeBucket*	buckets;
<<default Solver constructor>>=
    buckets = nil;
@ %def buckets
<<Node Bucket methods>>=
    void	dumpBucket();
@ {\em Note: this is in the `wrong' chunck Solver rather than bucket.}
<<Solver method implementations>>=
void NodeBucket::dumpBucket()
{
    cout << "head = " << head;
    if (head != nil) {
	cout << " (" << head->getId() << "), tail = " 
	     << tail << " (" << tail->getId() << ")" << endl;
    } else {
	cout << ", tail = " << tail << endl;
	return;
    }
    for (NodePtr n = head; n != nil; n = n->getNext()) {
	cout << n->getId() << "  ";
    }
    cout << endl;
}
@
Because we process the lowest labeled branch, we need a way to find
the strong branch quickly.  At the moment, we just search the list
sequentially looking for the lowest labeled, non-empty bucket.  To
speed this a bit, we keep track of the lowest labeled bucket we've
seen.  In the future, this simple array of buckets could be replaced
by some sort of heap.

Also, there are a couple of minor instances where we want to scan all 
of the buckets.  In those cases, having the maximum label that we
have seen is useful to avoid scanning every bucket.
<<Solver private members>>=
    NodeLabel 	lowestLabel;
    NodeLabel 	highestLabel;
<<default Solver constructor>>=
    lowestLabel = highestLabel = 0;
@ %def lowestLabel highestLabel
Adding a branch just requires finding the bucket based on the branch's
label, and inserting it into the list.  
<<Solver methods>>=
    virtual void addStrongBranch(Node& root);
<<Solver method implementations>>=
void PhaseSolver::addStrongBranch(Node& root)
{
    assert((root.getParentNode() == nil) && (root.getExcess() >= 0));
    assert(root.getLabel() < numNodes);
    CHECK_TREE(root, nil);
    NodeLabel label = root.getLabel();

    if (branchInBucket(root) == FALSE) {
	// actually add the branch
	(this->*addBranchFunc)(root);
	checkLowestLabel(label);
    }
}
@ %def addStrongBranch
Determining if a branch is already in a bucket
is a little tricky because we don't have a flag that to tell
us directly if the branch is already in the bucket.  Instead, we
will just look at the [[next]] pointer.  If the pointer is non-null,
then it must already be in a bucket.  However, the pointer might
be null if it's the last node in the bucket, so we have to check
for that too.  If it is in the bucket, we'll check its label against
the label of the nodes already in the bucket.
<<Solver methods>>=
    Boolean branchInBucket(Node& root);
<<Solver inline implementations>>=
INLINE Boolean PhaseSolver::branchInBucket(Node& root)
{
    Boolean result = FALSE;
    if ((root.getNext() != nil) ||  
        (buckets[root.getLabel()].getTail() == &root))
    {
	result = TRUE;
	assert(
	    (buckets[root.getLabel()].getHead()->getLabel() == root.getLabel())
	    || (buckets[root.getLabel()].getHead()->getParentNode() != nil)
	);
    }

    return result;
}
@ %def branchInBucket
We keep track of the lowest label every time a new branch is put
in a bucket.
<<Solver methods>>=
    void checkLowestLabel(NodeLabel label);
<<Solver inline implementations>>=
INLINE void PhaseSolver::checkLowestLabel(NodeLabel label)
{
    if (label < lowestLabel) {
	lowestLabel = label;
    } else if (label > highestLabel) {
	highestLabel = label;
    }
}
@ %def checkLowestLabel
<<Solver protected members>>=
    AddBranchPtr addBranchFunc;
<<default Solver constructor>>=
    addBranchFunc = &PhaseSolver::addBranchFifo;
@ %def addBranchFunc
<<Solver methods>>=
    void addBranchFifo(Node& root);
    void addBranchLifo(Node& root);
    void addBranchWave(Node& root);
<<Solver method implementations>>=
void PhaseSolver::addBranchFifo(Node& root)
{
    buckets[root.getLabel()].insertTail(root);
}
void PhaseSolver::addBranchLifo(Node& root)
{
    buckets[root.getLabel()].insertHead(root);
}
@ %def addBranchFifo addBranchLifo
The ``wave'' branch management is rather weak: it looks to
see if the branch root we are adding is the same as the
branch root we started processing.  If so it puts the
branch at the head of the list.  This is lame because what
we really want to know is if this branch is the same
branch as we are processing.  If we just did a merger with
it, it was probably rehung so that the root is no longer
the same. 
<<Solver method implementations>>=
void PhaseSolver::addBranchWave(Node& root)
{
    if (lastRoot == &root) {
	buckets[root.getLabel()].insertHead(root);
    } else {
	buckets[root.getLabel()].insertTail(root);
    }
}
@ The last branch root returned by [[getLowestBranch]].
<<Solver protected members>>=
    NodePtr lastRoot;
<<default Solver constructor>>=
    lastRoot = nil;
@ %def lastRoot
As the algorithm runs, we need to remove the lowest labeled branch.
First we need to locate the lowest labeled bucket, then we need to
remove the head of the list.  If there are no more strong branches,
we just return [[nil]].

The search always starts at the [[lowestLabel]] bucket.  As we
scan buckets, we increment [[lowestLabel]] until we find a 
non-empty bucket.
<<Solver protected members>>=
    NodePtr getLowestBucket();
<<Solver inline implementations>>=
INLINE NodePtr PhaseSolver::getLowestBucket()
{
    NodePtr result = nil;
    while (lowestLabel <= highestLabel) {
	result = buckets[lowestLabel].removeHead();
	if (result != nil) {
	    result->setNextNil();
	    break;
	} else {
	    lowestLabel++;
	}
    }

    lastRoot = result;
    return result;
}
@ %def getLowestBucket
The above method does the real work, but we have a virtual 
wrapper function to allow us to modify the behavior for the
Simplex solver.  Here in the base class, we just check the label.
<<Solver methods>>=
    virtual NodePtr getLowestBranch();
<<Solver method implementations>>=
NodePtr PhaseSolver::getLowestBranch()
{
    NodePtr result = getLowestBucket();
    assert((result != nil) ? (result->getLabel() == lowestLabel) : 1);
    return result;
}
@ %def getLowestBranch
\subsection{Delayed Normalization}
An alternative solving method involves `delayed normalization'.
In this case, we process the branches as before, but during a
merger, the tree is not totally renormalized.  In particular,
we do not push excess up the weak branch.  This leaves some
`pseudo-weak' branches, branches that we treat as weak, but they
have excess in them that may, in fact, make them strong.  
At the end of a phase, we need to scan for
deferred nodes and complete normalization on them.  This
may lead to the creation of more strong branches, in which case
we need to continue executing the algorithm.
<<Solver methods>>=
    void delayedNormalizeSolve(AddBranchPtr addFunc);
<<Solver method implementations>>=
void PhaseSolver::delayedNormalizeSolve(AddBranchPtr addFunc)
{
    addBranchFunc = addFunc;
    renormalizeFunc = &PhaseSolver::strongOnlyRenormalize;
    initGlobalRelabel();

    int currentPhase = 0;
    NodePtr strongBranch = nil; 
    NodeLabel strongLabel =  0;

    while (TRUE) {
	<<get strong branch - normalize if needed>>
	<<terminate early based on counting labels>>
	processBranch(*strongBranch);
    }
}
@ %def delayedNormalizeSolve
To get the next strong branch, we call [[getLowestBranch]], but that
might return [[nil]] if the only remaining `strong' branches are
still weak due to deferred normalization.  In that case, we perform
the deferred normalizations and get the lowest branch again.
<<get strong branch - normalize if needed>>=
    strongBranch = getLowestBranch(); 
    if (strongBranch == nil) {
	performDeferredNormalizations();
	<<get non-null branch>>
    }
@ 
If we get [[nil]] again, we really are done and should break out
of the loop.
<<get non-null branch>>=
	strongBranch = getLowestBranch();
	if (strongBranch == nil) {
	    break;
	}
@
Once we have the branch, we look to see if its label is different
than the current phase - i.e. are we trying to go to a new (higher)
phase.  If so, we put the branch back and do the renomalizations,
which might lead to a lower phase, which is why we put it back.
<<get strong branch - normalize if needed>>=
    strongLabel =  strongBranch->getLabel();
    if (strongLabel != currentPhase) {
	addStrongBranch(*strongBranch);	// should addHead
	performDeferredNormalizations();
	checkForRelabel();
@
Now, we try again to get a new branch.  If the label is 
different than the current phase, then we really are starting
a new phase.
<<get strong branch - normalize if needed>>=
	<<get non-null branch>>
	strongLabel =  strongBranch->getLabel();
	if (strongLabel != currentPhase) {
	    currentPhase = strongLabel;
	    TRACE( trout << "Phase " << currentPhase << endl;)
	}
    }
@ 
During the normal merger function, we enable deferred normalization
by using the follow function that will only renormalize the strong
branch.  This will leave excess at the weak node.  The weak branch
will still be considered weak so that strong nodes can still merge
to it.  
<<Solver methods>>=
    void strongOnlyRenormalize(Node& strongRoot, Node& weakNode);
<<Solver method implementations>>=
void PhaseSolver::strongOnlyRenormalize(Node& strongRoot, Node& weakNode)
{
    strongPush(strongRoot, weakNode);
    if (weakNode.flagIsSet(DEFERRED) == FALSE) {
	weakNode.setFlag(DEFERRED);
	weakNode.next2 = deferredNodes;
	deferredNodes = &weakNode;
    }
}
@ %def strongOnlyRenormalize
<<Solver data>>=
    NodePtr deferredNodes;
    static const int DEFERRED = 0x10;
<<default Solver constructor>>=
    deferredNodes = nil;
@ %def deferredNodes
To perform the delayed normalizations, we run down the list
of deferred nodes.  For each node, we perform a complete weak push
up to the weak root.  Note that it is possible for multiple nodes
in a branch to be weak (actually, we're hoping for that).  This means
that a deferred push could make a branch strong, and later we'll be
doing another push to the same branch - i.e. a `weak' push within a
strong branch.  

This is fine, but it does mean that we cannot use
the same `next' pointer to build the list of deferred weak nodes
as we use for building the list of strong nodes.  A deferred weak node
(still in the list of weak nodes) could be made into a strong branch
root due to a push from a descendent node, which means it needs to be
in the strong list.
<<Solver methods>>=
    Boolean performDeferredNormalizations();
<<Solver method implementations>>=
Boolean PhaseSolver::performDeferredNormalizations()
{
    int numWeakPushes = 0;
    int numSplittingPushes = 0;

    NodePtr currNode = deferredNodes;
    while (currNode != nil) {
	deferredNodes = currNode->next2;
	currNode->next2 = nil;
	<<normalize deferred node>>
    }

    deferredNodes = nil;

    TRACE( trout << "Performed " << numWeakPushes << " pushes with " 
		 << numSplittingPushes << " splitting pushes"  << endl; )

    return (numSplittingPushes > 0) ? TRUE : FALSE;
}
@ %def performDeferredNormalizations
<<normalize deferred node>>=
    if (currNode->flag != 0) {
	if (currNode->getExcess() > 0) {
	    numWeakPushes++;
	    TRACE( trout << "deferred push from node " << currNode->getId() 
			 << " label " << currNode->getLabel() << endl;)
	    if (weakPush(*currNode)) {
		numSplittingPushes++;
	    }
	} else {
	    TRACE( trout << "no deferred push from node " << currNode->getId() << endl; )
	}
	currNode->flag = 0;
    } 
    currNode = deferredNodes;
@ 
There is another strategy for performing deferred normalizations that
we have not yet implemented.  The basic idea is to try to push in parallel
from the leaves up to the root.  What we'd like to acheive is that
the excess from two parallel paths would merge at one node and we'd
then only have to do one push from that node up to the root.

We cannot really implement that directly.  We can approximate it by 
maintaining another set of buckets indexed by node label.  We process
it from maximum label down to zero.  We push excess from a node
up along the path to the root so long as the parent has the same
label.  As soon as we push flow up to a new labeled node, we queue
that node to be processed later when we get to that label.  This 
scheme might not detect the merger of two paths immediately, but it
would be detected as soon as we crossed into a new label.

This scheme might be an improvement due to coalescing the excess
from multiple paths into a single push up to the root.  On the
other hand, it involves a fair amount of overhead to be queueing
and dequeueing the nodes all the time.  Not to mention, it's more
compilicated, of course.

\subsection{Highest Root Label}
Here is a totally different solving function.  It is inspired by the
highest label variant of push-relabel.  Due to our tree/branch and label
structure, we cannot exactly perform a highest label on a per-node basis
the way push-relabel does.  For example,
if we choose the highest labeled node in a branch
and increase its label arbitrarily after an empty scan, then things will
fail.  If we loose the bound on the difference of node labels in the
graph, then we can't bound the maximum label of a node (invariant 1).  It
would seem strange to have strong nodes with residual capacity between
them, but widely different labels.  If we try to prune a node, but other
nodes in the branch have access to weak nodes, then the node should still
be active.  

So, instead what we do is process the branch with the highest labeled
root.  We process it in the usual way - from the root down only looking at
nodes with label $\ell$.  If we do not merge, we will relabel the top part
of the branch and put it back in the bucket list.  This will surely be the
highest labeled branch, so we will process it again.  

At some point, we will achieve a condition similar to the main algorithm's
early termination - i.e. there will be no nodes in the tree with label
one less than ours.  We cannot terminate the algorithm at this time, but
we can prune the branch (subject to some conditions discussed below).

For this method, we do not support delayed normalization, and we did not
even consider implementing it for Simplex.  Therefore, it is mutually
exclusive with delayed normalization and simplex.
<<Solver methods>>=
    void highestLabelSolve(AddBranchPtr addFunc);
<<Solver method implementations>>=
void PhaseSolver::highestLabelSolve(AddBranchPtr addFunc)
{
    IFTRACE(int currentPhase = 0;)
    addBranchFunc = addFunc;
    renormalizeFunc = &PhaseSolver::fullRenormalize;
    initGlobalRelabel();

    NodePtr strongBranch = getHighestBranch();
    while (strongBranch != nil) {
	NodeLabel strongLabel =  strongBranch->getLabel();
	IFTRACE(
	    if ( strongLabel != currentPhase) {
		currentPhase = strongLabel;
		TRACE(trout << "Phase " << currentPhase << endl;);
	    }
	)
	<<prune gap branch if needed>>
	Boolean performedMerger = processBranch(*strongBranch);
	STATS(performedMerger ? 0 : numEmptyBranchScans++ );
	checkForRelabel();
	strongBranch = getHighestBranch();
    }
    IFDEBUG(checkMergers());
}
@ %def highestLabelSolve
If there is a gap, we can prune this branch and all others with the same
label or higher.  (Actually, we're only pruning this branch.  We
could/should prune all nodes with the same label, as push-relabel does.)

Note that this is a little tricky when we deal with something like
saturate-all with distance to sink labeling.  This creates weak nodes with
fairly high labels.  We can't simply look for no nodes with label 
$\ell - 1$ because after we've processed some strong nodes with high
labels, we could get down to some that have labels less than some weak
nodes out there.  Therefore, during initializaiton, we need to figure out
the highest labeled weak node and avoid pruning until we reach that level.

Another alternative would be to scan the label counts from [[strongLabel]]
up to [[maxWeakLabel]].  If there are no nodes in that range, we could
safely prune this branch without waiting for its label to exceed
[[maxWeakLabel]].  On the one hand, that would involve an expensive
iterative test here.  On the other hand, midlessly raising the label of
this strong branch above [[maxWeakLabel]] is expensive.  Furthermore, if
we make that iterative scan and it comes up empty, we could reduce
[[maxWeakLabel]] to [[strongLabel]] thus reducing its overhead in the
future.
<<prune gap branch if needed>>=
    if ((labelCount[strongLabel - 1] == 0) && (strongLabel > maxWeakLabel))
    {
	TRACE(trout << "Found a gap at label " << strongLabel << endl);
	setBranchLabel(*strongBranch, numNodes);
	strongBranch = getHighestBranch();
	if (strongBranch == nil) {
	    break;
	}
	strongLabel =  strongBranch->getLabel();
    }
@
The first, obvious thing we need for the highest labeled root solver, is a
function to fetch the highest labeled branch.  Confusingly, we will
override the virtual function [[getLowestBranch]] to do this.
<<Solver methods>>=
    NodePtr getHighestBranch();
<<Solver method implementations>>=
NodePtr PhaseSolver::getHighestBranch()
{
    NodePtr result = nil;
    <<find highest non-empty bucket>>
    assert((result != nil) ? (result->getLabel() == highestLabel) : 1);
    return result;
}
@ %def getHighestBranch
Finding the highest labeled non-empty bucket, is basically the reverse
of [[getLowestBucket]]: search from [[highestLabel]] down to
[[lowestLabel]].
<<find highest non-empty bucket>>=
    while(highestLabel > lowestLabel) {
	result = buckets[highestLabel].removeHead();
	if (result != nil) {
	    result->setNextNil();
	    break;
	} else {
	    highestLabel--;
	}
    }

    lastRoot = result;
@
\subsection{Simplex Merger}
Another, more involved method of solving is using a `simplex-style'
merger function.  The simplex merger avoids multiple splits during
the merger computing the bottleneck capacity along the cycle
from the strong root, through the merger arc, and up to the weak
root.  We only push excess equal to this bottleneck capacity, so
there will be at most one split during the merger, in most cases
the strong root will remain strong.  Note that due to the complexity
of all of this, we do not support delayed normalization.

As we shall see, this is a pretty significant deviation from the
code we have develeoped above, we will implement this  as a sub-class
of [[PhaseSolver]] rather than relying on tricks with pointers to
functions.
<<Simplex declaration>>=
class SimplexSolver : public PhaseSolver
{
    public: <<Simplex public declarations>>
    private: <<Simplex private declarations>>
};
@ %def SimplexSolver
\subsubsection{Simplex merge function}
<<Simplex public declarations>>=
    virtual void merge(Node& strong, Node& weak, Edge& edge);
<<Simplex implementations>>=
void SimplexSolver::merge(Node& strong, Node& weak, Edge& edge)
{
    TRACE( trout << "simplexMerge " << strong.getId() << " ("
		   << strong.getRootExcess() << ") to " 
		   << weak.getId() << " ("
		   << weak.getExcess() << ", "
		   << weak.getRootExcess() << "), cap "
		   << edge.residCapacity(strong) << ", s-label "
		   << strong.getLabel() << endl;)

    <<simplex merge function>>
}
@ %def merge
Compute the bottleneck capacity in the strong and weak branches.
<<simplex merge function>>=
    FlowAmount strongCapacity, weakCapacity;
    NodePtr strongChild, weakChild;
    Node& strongRoot = findBottleneckArc(strong, TRUE, strongCapacity, strongChild);
    Node& weakRoot = findBottleneckArc(weak, FALSE, weakCapacity, weakChild);
    FlowAmount mergerCapacity = edge.residCapacity(strong);
    FlowAmount excess = strongRoot.getExcess();
    TRACE( trout << "strongCapacity=" << strongCapacity <<
                    ", mergerCapacity=" << mergerCapacity <<
		    ", weakCapacity=" << weakCapacity << endl;);
@
First, let's address a special case that seems to come up a lot
with simplex: zero-excess root.  Initially, we used to filter them
out via [[getLowestBranch]] and [[addStrongBranch]], but that leads
to weak branches with roots that have label greater than one.
Therefore, we'll just merge it to a weak branch without worrying 
about pushing any flow around.
<<simplex merge function>>=
    if (excess == 0) {
	<<fix degenerate flags for whole strong branch>>
	strong.rehangWeak();
	weak.addChild(strong, edge);
	CHECK_TREE(weakRoot, nil);
    }
@
Figure out where the bottleneck is.  If there is less excess than
the bottlenck, we will just perform a normal merger.
<<simplex merge function>>=
    else if ((excess < strongCapacity) && (excess < mergerCapacity) && 
        (excess < weakCapacity))
    {
	// normal merger without splits
	<<fix degenerate flags for whole strong branch>>
	mergeStrongBranch(strong, weak, edge);
    } else if ((strongCapacity <= mergerCapacity) && 
               (strongCapacity <= weakCapacity))
    {
	<<strong side bottleneck>>
    } else if ((strongCapacity > mergerCapacity) &&
               (mergerCapacity <= weakCapacity))
    {
	<<merger arc bottleneck>>
    } else {
	<<weak side bottleneck>>
    }
@
$\bullet$ If the bottleneck is on the strong side, we need to pull as much flow
down from the strong root as possible (i.e. the bottleneck capacity).
<<strong side bottleneck>>=
    assert(strongChild != nil);
    assert(strongChild->getParentDownCapacity() == strongCapacity);
    pullFromRoot(*strongChild, strongCapacity);
    assert(strongChild->getExcess() == strongCapacity);
    NodePtr newRoot = findNewRoot(strongRoot, *strongChild);
    strongChild->split();
    CHECK_TREE(strongRoot, nil);
@ We need to fix up the degenerate branch flags, which will
be explained later.
<<strong side bottleneck>>=
    <<fix degenerate flags in strong branch>>
@ The call above to [[findNewRoot]] computes which node, if any,
from the strong branch needs to be re-inserted in the buckets.
<<strong side bottleneck>>=
    if (newRoot != nil) {
	addStrongBranch(*newRoot);
    }
@ The remaining fragment of the strong branch can be merged with
the weak node, and we can will let our variant of [[merge]] in the 
base class push from the child node (which is now the root of 
the fragment) all the way up to the weak root.
<<strong side bottleneck>>=
    mergeStrongBranch(strong, weak, edge);
@
$\bullet$ When the merger arc is the bottleneck, the processing 
is similar except that
we don't have to merge a fragment of the strong branch to the weak
branch.  We pull down as much flow as we can to the strong node. 
<<merger arc bottleneck>>=
    pullFromRoot(strong, mergerCapacity);
    assert(strong.getExcess() >= mergerCapacity);
@ Push the flow to the weak node (saturating the merger arc). 
<<merger arc bottleneck>>=
    edge.setDirectionTo(&weak);
    edge.increaseFlow(mergerCapacity);
    strong.decrementExcess(mergerCapacity);
    weak.incrementExcess(mergerCapacity);
    CHECK_TREE(strongRoot, nil);
@ Then we insert the last strong root and
perform a weak push (which won't split).
We know that the last root is the correct branch to insert (instead
of calling [[findNewRoot]]) because the merger is happening at or
below the last root, and it's still strong because the merger arc
is the bottleneck.
<<merger arc bottleneck>>=
    addStrongBranch(*lastRoot);
    weakPush(weak);
    CHECK_TREE(weakRoot, nil);
@
$\bullet$ Finally, when the bottleneck arc is on the weak side, the code
is rather unique.  We want to end up with the weak node dangling
from the strong node, which is the reverse of how we usually
think of mergers.  Again we will start by pulling flow down to the strong
node. 
<<weak side bottleneck>>=
    pullFromRoot(strong, weakCapacity);
    assert(strong.getExcess() >= weakCapacity);
@ Next, we will push it over the merger arc to the weak node.
Then, we push it up to the bottleneck child on the
weak side using [[strongPush]], even though it's the weak
side of the tree.
<<weak side bottleneck>>=
    edge.setDirectionTo(&weak);
    edge.increaseFlow(weakCapacity);
    strong.decrementExcess(weakCapacity);
    weak.incrementExcess(weakCapacity);
    strongPush(weak, *weakChild);
@ Now, we push the flow across the bottleneck arc, which we can just use
[[pushToParent]] since the pointers parent-child relationship is
correct.  However, we still need to split ourselves.
<<weak side bottleneck>>=
    NodePtr weakParent = weakChild->getParentNode();
    assert(weakParent != nil);
    //STATS(numPushToParent++);
    weakChild->pushToParent();
    weakChild->split();
    assert(weakChild->getExcess() == 0);
@ At this point, we have excess in the amount of the bottleneck capacity
sitting on the weak parent node, so we can easily push it up to
the weak root without any splitting.  The [[weakPush]] method will
also take care of the case where we push enough excess to the root
that the branch becomes strong.
<<weak side bottleneck>>=
    weakPush(*weakParent);		// no split
    CHECK_TREE(weakRoot, nil);
@ Finally, we need to clean things up.  The weak fragment from [[weak]]
to [[weakChild]] needs to be rehung and attached under the strong node.
And then the strong node needs to be re-inserted into the strong buckets.
<<weak side bottleneck>>=
    weak.rehangWeak();
    strong.addWeakChild(weak, edge);
    <<set degenerate flags in weak branch>>
    CHECK_TREE(strongRoot, nil);
    addStrongBranch(*lastRoot);
@ 
In the code above, we used to call the base class [[merge]] function
to merge part or all of the strong branch to the weak branch.  
However this causes problems when we've been dealing with degenerate
branches.  In the base class, we only rehang trees in the strong
branch, so we can exploit the `current element' in the children
arrays to speed rehanging and subsequent searches of the strong
branches.  With degenerate branches, this breaks down.  

Therefore, we have a copy of the base class merge function with
different calls to tree manipulation functions.  Also, we 
hard-code the renomalization function to renormalize the whole tree.
<<Simplex public declarations>>=
    void mergeStrongBranch(Node& strong, Node& weak, Edge& edge);
<<Simplex implementations>>=
void SimplexSolver::mergeStrongBranch(Node& strong, Node& weak, Edge& edge)
{
    Node& strongRoot = *strong.rehangWeak();
    weak.addChild(strong, edge);
    CHECK_TREE(*weak.getRoot(), nil);
    // renormalize the tree
    fullRenormalize(strongRoot, weak);
}
@ %def mergeStrongBranch
\subsubsection{Degenerate branches}
\label{degenerate-branches}
The code above for the three classes of mergers looks 
plausible, but there
is a serious problem during a merger when the bottleneck is on
the weak side.  Suppose during phase $\ell$, the strong node
is $s$, the weak node is $w$, and the bottleneck arc is $(u,v)$,
where $u$ is closest to $w$.  We want to suspend the fragment of
the weak branch below the bottleneck from $w$, and attach it to
the strong node, $s$, via the merger arc.  This violates the
monotoicity of paths from the strong root because we know that $w$
has label $\ell - 1$, while $s$ has label $\ell$.  The other weak
nodes have labels less than or equal to $\ell - 1$.

In terms of algorithmic complexity, this violation of 
monotonicity is not a
problem on the strong side, but it is a problem in our implementation
because we count on being able to begin scanning a strong branch
(during phase $\ell$) from the root and knowing that labels only
increase with depth so that when we encounter a node with a higher
label (say $\ell + 1$), we can stop scanning that sub-tree during
phase $\ell$.

We will accomodate this by introducing the concept of {\em degenerate
sub-trees}.  After a simplex merger with the bottleneck arc in the
weak branch, we rehang the weak fragment from $w$ and attach it to
$s$.  We call this branch `degenerate' because we know that the
label monotoicity has been violated.  A merger that creates a
degenerate branch is also called degenerate.  We know that all the
nodes along the path from $w$ to $u$ have labels less than $\ell$
that are monotically decreasng.  There may also be descendent nodes
of the nodes along the path with labels less than $\ell$, but they
will maintain monotonicity within their local sub-trees.

We need to process these nodes and their descendents before we
process any of the nodes with label $\ell$ again.  (Note that all
nodes along the path from $s$ up to the strong root have label
$\ell$ because we were in phase $\ell$ at the time of the merger.)

We process these nodes from the leaf node, $u$, up long the path
of parent pointers until we get back to $s$.  As we process each
of these nodes and their children, we will be either merging or
relabeling the nodes.  If we reach $s$ without merging, we will
know that all nodes below $s$ have label $\ell$ or more because
when we cannot find a merger for a node, we increment its label.
At this point, monotonicity has been restored (it is no longer
degenerate) and we can place the branch into a strong bucket to
continue processing during phase $\ell$.

To accomplish this, we will treat each node along the path
from the weak child up to the weak node as the root of a 
strong branch.  We can process these sub-trees to yield
mergers or increase the label.

This is the final step in the merger code with the bottleneck
in the weak branch.
Although we could do this inline (especially with the aid
of [[noweb]], we will use a separate function so that we
can profile the performance of this function.
<<set degenerate flags in weak branch>>=
    queueDegeneratePath(*weakChild, weak);
<<Simplex private declarations>>=
    void queueDegeneratePath(Node& first, Node&last);
@
This function will involve tracing up the path from the
first node to the last node adding each node as a new
strong root.  
<<Simplex implementations>>=
void SimplexSolver::queueDegeneratePath(Node& first, Node& last)
{
    NodePtr currNode = &first;
    while (currNode != &last) {
	addDegenerateBranch(*currNode);
	currNode = currNode->getParentNode();
    }
    addDegenerateBranch(last);
}
@ %def queueDegneratePath
Adding degenerate branches is similar to [[addStrongBranch]]
except that the assumptions are very different - e.g. we're not
dealing with root nodes, and a degenerate branch better not 
already be in a bucket.  Furthermore, we want to mark degenerate
nodes to keep track of them if they get merged to a weak branch.
One subtle, yet significant difference is that we always want to
add degenerate roots to the tail of the bucket.  This way if
a node and its parent both have the same label, we will process
the child before the parent - i.e. move up along the path
towards the original strong node.
<<Simplex private declarations>>=
    void addDegenerateBranch(Node& root);
<<Simplex implementations>>=
void SimplexSolver::addDegenerateBranch(Node& root)
{
    assert(isStrongNode(root) == FALSE);
    NodeLabel label = root.getLabel();
    assert(root.getNext() == nil);

    // mark the branch and add it
    assert(root.flag == 0);
    root.setFlag(DEGENERATE_SUBTREE);
    CHECK_TREE(root, root.getParentEdge());
    addBranchFifo(root);

    checkLowestLabel(label);
}
@ %def addDegenerateBranch
<<Simplex private declarations>>=
    static const int DEGENERATE_SUBTREE = 4;
@ %def DEGENERATE_SUBTREE
So, now that we have degenerate branches in the buckets, we need
to deal with them.  The first issue is getting them out.  The 
problem is that between the time we added a degenerate node and
when we find it as a strong root, there might have been a merger
that `demoted' it to weak.  In this case, we need to ignore it
and continue looking.

One subtle case to be aware of is when we queue the degenerate branch,
we queue the root of the entire branch as well as the degenerate
nodes along the path below [[strong]].  If there is a subsequent merger
where there is no bottleneck, the entire strong branch will be
rehung and made weak.  The root for the entire strong branch will
still be in its bucket.  However, now it will no longer be a root,
so we will know to ignore it.
<<Simplex public declarations>>=
    virtual NodePtr getLowestBranch();
<<Simplex implementations>>=
NodePtr SimplexSolver::getLowestBranch()
{
    NodePtr result = getLowestBucket();
    while (result != nil) {
	if (result->isRootNode()) {
	    // a normal strong root - may have zero exess
	    break;
	} else if (result->flagIsSet(DEGENERATE_SUBTREE)) {
	    // found a degenerate root that is still strong
	    break;
	}
	result = getLowestBucket();
    }

    return result;
}
@ %def getLowestBranch
Once we get the lowest labeled branch, we will need to process it.
This is basically the same as we do in the base class, except that
we want to keep an eye out for degenerate sub-trees.  After we
process on of these, if there were no mergers (i.e. it was relabeled),
we do not want to put it back in the bucket because we know its
monotonicity has been restored.
<<Simplex public declarations>>=
    virtual Boolean processBranch(Node& root);
<<Simplex implementations>>=
Boolean SimplexSolver::processBranch(Node& root)
{
    Boolean performedMerger = FALSE;
    degenerateBranch = root.flagIsSet(DEGENERATE_SUBTREE);

    if (labelCount[root.getLabel() - 1] == 0) {
	TRACE( trout << "Relabel branch " << root.getId() 
	             << " with label " << root.getLabel() << endl; )
	relabelSubtree(root);
	if (degenerateBranch) {
	    root.clearFlag(DEGENERATE_SUBTREE);
	} else {
	    addStrongBranch(root);	
	}
    } else {
	performedMerger = processSubtree(root);
	if (!performedMerger) {
	    if (degenerateBranch) {
		root.clearFlag(DEGENERATE_SUBTREE);
	    } else {
		addStrongBranch(root);	
	    }
	}
    }
    validateDegenerateBranches();
    return performedMerger;
}
<<Simplex private declarations>>=
    Boolean degenerateBranch;
@ %def processBranch
The code above creates degenerate branches during mergers with
wthe bottleneck in the weak branch.  One tricky complication
is if we have a merger from a degenerate branch where the 
bottleneck arc is within the strong, degenerate branch, some
of the nodes we flagged and put into buckets could become
weak.  In which case, we no longer want to process them as
strong branches.

Rather than digging through the buckets trying to pull these nodes
out, we will simply clear their degnerate flag.  We will do
this after the bottleneck arc has been saturated but before the
strong branch has been merged and rehung.  Therefore, we want to
trace the path from [[strong]] up to [[strongChild]] clearing
the degenerate flag.
({\em We could be a tad more efficient about this because we
know that the degenerate nodes will be contiguous along the
path, but there might be some non-degenerate nodes at the
begining of the path.  We could quit looking upon reaching the
end of the degenerate portion of the path.})
<<fix degenerate flags in strong branch>>=
    if (degenerateBranch) {
	strong.clearFlagPath(DEGENERATE_SUBTREE, strongChild);
    }
@ Similarly, when there is no bottleneck arc, and the entire 
strong branch becomes weak, we need to clear the degenerate
flags for the whole path from the strong merger node up to
the strong root.  We do this before performing the merger, so
[[strong]] is at the bottom of the path and [[strongRoot]] is
at the top.
<<fix degenerate flags for whole strong branch>>=
    if (degenerateBranch) {
	strong.clearFlagPath(DEGENERATE_SUBTREE, nil);
    }
@
In addition to tweaking the degenerate flags in the strong branch,
we need to figure out what node to queue in a bucket as a strong
root.  This is non-trivial to figure out because need to to
figure out where the breakpoint arc is in relation to the node
that we began processing via [[processBranch]].
We assume that this is being invoked before the strong branch
has been split at the bottleneck arc.
<<Simplex public declarations>>=
    NodePtr findNewRoot(Node& strongRoot, Node& strongChild);
<<Simplex implementations>>=
NodePtr SimplexSolver::findNewRoot(Node& strongRoot, Node& strongChild)
{
    <<find new root>>
}
@ %def findNewRoot
If our search started at the strong root, we know we were processing 
a non-degenerate branch and that the strong root is where to continue
the search.  
<<find new root>>=
    if (lastRoot == &strongRoot) {
	return &strongRoot;
    }
@ If we started our search at the strong child, the we want to
contine the search at the parent (if its degenerate) or the root
of the branch (if its not).  However, both of these nodes should 
already be in a bucket from [[queueDegeneratePath]] or plain old
[[addStrongBranch]] for the root of a degenerate branch.  In this
case, we return [[nil]] to signal that no branch needs to be
queued.
<<find new root>>=
    if (lastRoot == &strongChild) {
	return nil;
    }
@ With those easy cases out of the way, we need to scan up from
the the last root searching for the bottleneck arc (actually
the parent node above the arc).  
If we don't find the arc, then we'll continue from the last
root.  Otherwise, it depends if the parent node is degenerate or not.
<<find new root>>=
    NodePtr result = lastRoot;
    NodePtr parent = strongChild.getParentNode();
    NodePtr currNode = lastRoot->getParentNode();
    while (currNode != nil) {
	if (currNode == parent) {
	    if (currNode->flagIsSet(DEGENERATE_SUBTREE)) {
		if (branchInBucket(*currNode)) {
		    result = nil;
		} else {
		    result = currNode;
		    TRACE( trout << "Found degenerate branch root that "
				    "is not already in bucket: " 
				 << result->getId() << endl; );
		}
		break;
	    } else {
		result = &strongRoot;
		break;
	    }
	}
	currNode = currNode->getParentNode();
    }

    return result;
@
Since it seems that we are implementing special versions of just
about every method that touches branches, let's add a new
version of [[addStrongBranch]].  This is mostly to verify the
conditions that we expect to hold for the Simlex version - basically
keeping an eye on degenerate branches.
<<Simplex public declarations>>=
    virtual void addStrongBranch(Node& root);
<<Simplex implementations>>=
void SimplexSolver::addStrongBranch(Node& root)
{
    NodeExcess excess = root.getExcess();
    if (excess > 0) {
	assert(root.flagIsSet(DEGENERATE_SUBTREE) == FALSE);
	PhaseSolver::addStrongBranch(root);
    } else {
	assert(excess == 0);
	if (root.getParentNode() != nil) {
	    assert(root.flagIsSet(DEGENERATE_SUBTREE));
	    <<re-insert a degenerate branch>>
	} else {
	    TRACE( trout << "Adding strong branch with zero excess: " 
			 << root.getId() << endl; );
	    PhaseSolver::addStrongBranch(root);
	}
    }
}
@ %def addStrongBranch
We need to be careful how we queue degenerate branches here.  They
are inserted in FIFO order for the first time in [[addDegenerateBranch]].
However, if this function is called to queue one, it must be from
the Simplex [[merge]] function, so we must be requeing a branch.
Therefore, we want to add it to the head of the list (i.e. LIFO)
so that it can be discharged before its parent, if the parent has
the same label.
<<re-insert a degenerate branch>>=
    assert(root.getNext() == nil);
    addBranchLifo(root);
    checkLowestLabel(root.getLabel());
@
\subsubsection{Utility functions}
The code above uses a method to find the bottleneck arc 
within the a branch.  The bottleneck capacity is the downwards
capacity in the strong branch and the upwards capacity in the 
weak branch.  This function returns the
root of the branch(like [[Node.rehang]]), the bottleneck capacity, and
the node on the `bottom' of the bottleneck arc.  The bottleneck arc
is the parent arc of the child node that is returned.
If the node is a root (i.e. has no parent), the bottleneck capacity 
will be [[MAXINT]] and the bottleneck child will be [[nil]].
If there are multiple arcs along the path with the same bottleneck
capacity, we will return the one closest to the root of a strong
branch or the further one in a weak branch.
<<Simplex private declarations>>=
    Node& findBottleneckArc(Node& node, 
			    Boolean isStrong,
			    FlowAmount& bottleneckCap, 
			    NodePtr& bottleneckChild);
<<Simplex implementations>>=
Node& SimplexSolver::findBottleneckArc(Node& node, 
				     Boolean isStrong,
				     FlowAmount& bottleneckCap, 
				     NodePtr& bottleneckChild)
{
    NodePtr currNode = &node;
    EdgePtr parentEdge = node.getParentEdge();
    bottleneckChild = nil;
    bottleneckCap = MAXINT;

    while (parentEdge != nil) {
	NodePtr parentNode = parentEdge->getOtherNode(currNode);
	if (isStrong) {
	    FlowAmount res = parentEdge->residCapacity(*parentNode);
	    if (res <= bottleneckCap) {
		bottleneckCap = res;
		bottleneckChild = currNode;
	    }
	} else {
	    FlowAmount res = parentEdge->residCapacity(*currNode);
	    if (res < bottleneckCap) {
		bottleneckCap = res;
		bottleneckChild = currNode;
	    }
	}

	currNode = parentNode;
	parentEdge = currNode->getParentEdge();
    }

    return (currNode == &node) ? node : *currNode;
}
@ %def findBottleneckArc

In the strong branch, we need a method to pull some of the excess
from the root down to a node.  This requires that there be sufficient
capacity in the reverse direction in each arc along the path.
The flow on each arc will be adjusted to reflect the flow that we
are pulling down.  
<<Simplex private declarations>>=
    void pullFromRoot(Node& node, FlowAmount amount);
<<Simplex implementations>>=
void SimplexSolver::pullFromRoot(Node& node, FlowAmount amount)
{
    NodePtr currNode = &node;
    EdgePtr parentEdge = node.getParentEdge();

    if (amount == 0) {
	TRACE( trout << "Pull zero flow from root to " << node.getId() << endl; );
	return;
    }
    if (parentEdge == nil) {
	// node is a root - the excess is already here: nothing to do
	return;
    }

    while (parentEdge != nil) {
	Node& parentNode = *parentEdge->getOtherNode(currNode);
	assert(parentEdge->residCapacity(parentNode) >= amount);
	parentEdge->decreaseFlow(amount);
	currNode = &parentNode;
	parentEdge = parentNode.getParentEdge();
    }
    <<update excesses at endpoints>>
}
@ %def pullFromRoot
The excess of the root node (pointed to by [[currNode]] will
decrease, and the excess of the node that we are starting from 
will increase.
<<update excesses at endpoints>>=
    assert(currNode->getExcess() >= amount);
    currNode->decrementExcess(amount);
    node.incrementExcess(amount);
@
Here is a debug function that validates the queues of degenerate
branches.
<<Simplex private declarations>>=
    void validateDegenerateBranches();
<<Simplex implementations>>=
void SimplexSolver::validateDegenerateBranches()
{
    for (int i = 1; i <= numNodes; i++) {
	Node& node = nodes[i];
	if (node.flagIsSet(DEGENERATE_SUBTREE)) {
	    assert(branchInBucket(node));
	    // look for same-label parent in same bucket
	    NodePtr parent = node.getParentNode();
	    assert(parent != nil);
	    if ((parent->getLabel() == node.getLabel()) &&
	        (parent->flagIsSet(DEGENERATE_SUBTREE)))
	    {
		NodePtr currNode = node.getNext();
		while (currNode != parent) {
		    assert(currNode != nil);
		    currNode = currNode->getNext();
		}
	    }
	}
    }
}
@ %def validateDegenerateBranches
\subsection{Establishing an Initial Normalized Tree}
There are a number of different ways that we can build an initial,
normalized tree.  Building the initial tree is totally independent
of the execution of the algorithm.  

\subsubsection{Simple Initialization}
At the very least, this simplest initialization
needs to saturate all source- and sink.adjacent edges and
source-adjacent nodes 2, and label the others 1.
classify the nodes as strong or weak.
<<Solver methods>>=
    void buildSimpleTree(LabelMethod labelMethod);
<<Solver method implementations>>=
void PhaseSolver::buildSimpleTree(LabelMethod labelMethod)
{
    saturateSourceSinkArcs();
    setLabelsAndStatus(labelMethod);
}
@ %def buildSimpleTree
The code to saturate the source- and sink-adjacent arcs is
broken out into a separate function because many schemes
for building the initial tree will want start by saturating
these arcs.  When we saturate an edge into the sink or out of the source, 
we remove it from the list
of arcs for the node.  This will prevent flow from going back to
either the source or the sink without having to special-case these
arcs at run-time.  Also, we want to deal with arcs going into
the source or out of the sink because these will never be used
in an optimal solution.
<<Solver methods>>=
    void saturateSourceSinkArcs();
<<Solver method implementations>>=
void PhaseSolver::saturateSourceSinkArcs()
{
    for(sourceNode->resetIterations();
        sourceNode->hasMoreNeighbors();
	sourceNode->advanceNeighbors())
    {
	Edge& edge = sourceNode->getCurrentNeighbor();
	<<saturate or remove current edge>>
    }

    for(sinkNode->resetIterations();
        sinkNode->hasMoreNeighbors();
	sinkNode->advanceNeighbors())
    {
	Edge& edge = sinkNode->getCurrentNeighbor();
	<<saturate or remove current edge>>
    }
}
@ %def saturateSourceSinkArcs
If the arc goes straight from the source to the sink, we just saturated it
(if we haven't already) and don't bother doing anything else.
<<saturate or remove current edge>>=
	if ((edge.getSource() == sourceNode) &&
	    (edge.getDest() == sinkNode))
	{
	    if (edge.residCapacity() > 0) {
		// we see these arcs twice: source and sink - only sat once
		edge.saturate();
	    }
	    continue;
	}
@ When we saturate an edge, we remove it from the list
of arcs for the source- or sink-adjacent node.  This will prevent flow 
from going back to
either the source or the sink without having to special-case these
arcs at run-time.  However, before performing flow recovery (in 
[[convertToFlow]]) we will need to add them back to return excess/deficit
to the source/sink.  Also, we want to remove the arcs going into
the source or out of the sink because these will never be used
in an optimal solution, but these do not have to be added back.
<<saturate or remove current edge>>=
	if (edge.getSource() == sourceNode) {
	    FlowAmount excess = edge.saturate();
	    Node& srcAdjNode = *edge.getDest();
	    srcAdjNode.removeNeighbor(edge);
	    srcAdjNode.incrementExcess(excess);
	    srcAdjNode.setFlag(SOURCE_ADJ);
	}  else if (edge.getDest() == sourceNode) {
	    // edge into source never used
	    edge.getSource()->removeNeighbor(edge);
	}
	if (edge.getDest() == sinkNode) {
	    FlowAmount excess = edge.saturate();
	    Node& sinkAdjNode = *edge.getSource();
	    sinkAdjNode.decrementExcess(excess);
	    sinkAdjNode.removeNeighbor(edge);
	    sinkAdjNode.setFlag(SINK_ADJ);
	} else if (edge.getSource() == sinkNode) {
	    // edge out of sink never used
	    edge.getDest()->removeNeighbor(edge);
	}
<<Solver data>>=
    static const int SOURCE_ADJ = 0x100;
    static const int SINK_ADJ = 0x200;
    static const int ADJACENT = SOURCE_ADJ | SINK_ADJ;
@
After we have built an initial tree structure, we need to assign labels to
the nodes, and we need to add strong nodes to buckets based on their
tables.  We currently support three different labeling schemes.  At this
point in the execution, we assume all label counts are zero.
<<Solver methods>>=
    void setLabelsAndStatus(LabelMethod labelMethod);
<<Solver method implementations>>=
void PhaseSolver::setLabelsAndStatus(LabelMethod labelMethod)
{
    switch(labelMethod)
    {
	case LABELS_CONSTANT:
	    setConstantLabels();
	    break;
	case LABELS_SINK_DIST:
	    setSinkDistLabels();
	    break;
	case LABELS_DEFICIT_DIST:
	    setDeficitDistLabels();
	    break;
	default:
	    assert("invalid labeling method" == nil);
    }
    setInitialLabelCounts();
}
@ %def setLabelsAndStatus
This simplest scheme is to give weak nodes label one and strong nodes
label two.\footnote {We could give strong nodes label one, but they'd have
no one to merge with until they are relabled to two.}
We iterate over the root nodes.  We label all nodes below a root with the
label based on the excess of the root.
Anything with positive excess is assumed to be
strong, and everything else is assumed to be weak.
Strong nodes are added as strong branches.
<<Solver methods>>=
    void setConstantLabels();
<<Solver method implementations>>=
void PhaseSolver::setConstantLabels()
{
    for (int i = 1; i <= numNodes; i++) {
	Node& node = nodes[i];
	if (node.isRootNode()) {
	    NodeLabel label = INIT_ZERO_LABEL;
	    if (node.getExcess() > 0) {
		label = INIT_STRONG_LABEL;
	    } else if (node.getExcess() < 0) {
		label = INIT_WEAK_LABEL;
	    }
	    labelBranch(node, label);

	    if (node.getExcess() > 0) {
		addStrongBranch(node);
	    }
	}
	node.resetIterations();
    }
}
@ %def setConstantLabels
<<Solver private members>>=
    static const int INIT_STRONG_LABEL=2;
    static const int INIT_WEAK_LABEL=1;
    static const int INIT_ZERO_LABEL=1;
@ %def INIT_STRONG_LABEL INIT_WEAK_LABEL INIT_ZERO_LABEL
This function sets the label of a node and all of its children.
It also updates the count of how many nodes have each label value.
<<Solver methods>>=
    void labelBranch(Node& node, NodeLabel label);
<<Solver method implementations>>=
void PhaseSolver::labelBranch(Node& node, NodeLabel label)
{
    node.setLabel(label);

    node.resetIterations();
    while (node.hasMoreChildren()) {
	Node& child = node.getCurrentChild();
	labelBranch(child, label);
	node.advanceChildren();
    }
    node.resetIterations();
}
@ %def labelBranch
about keeping track of the label counts.  This is especially hard if we
re-use code that was intended for use while solving when the labels are
correct vs. initialization when the labels are not yet set.  Therefore,
just to make sure everything is perfectly correct, we'll reset the label
counts and then scan the nodes to count them correctly.
Note that shortest path initialization ([[buildSpTree]]) 
does {\em not} use this method.
<<Solver methods>>=
    void setInitialLabelCounts();
<<Solver method implementations>>=
void PhaseSolver::setInitialLabelCounts()
{
    for (int i = 0; i <= numNodes; i++) {
	labelCount[i] = 0;
    }

    maxWeakLabel = maxInitialLabel = INIT_WEAK_LABEL;
    for (int i = 1; i <= numNodes; i++) {
	Node& node = nodes[i];
	if ((&node == sourceNode) || (&node == sinkNode)) {
	    continue;
	}

	NodeLabel l = node.getLabel();

	if (l < numNodes) {
	    labelCount[l]++;
	    if (l > maxInitialLabel) {
		maxInitialLabel = l;
	    }
	    if ((node.getExcess() < 0) && (l > maxWeakLabel)) {
		maxWeakLabel = l;
	    }
	} else {
	    labelCount[numNodes]++;
	}
    }
#ifdef DEBUG
    int total = 0;
    for (int i = 0; i <= numNodes; i++) {
	total += labelCount[i];
    }
    assert(total == (numNodes - 2));
#endif DEBUG
}
@ %def setInitialLabelCounts 
Above we computed a couple of maximum label values that are used by the
solver functions to control early termination. The [[maxInitialLabel]] is
pretty obvious.  The [[maxWeakLabel]] is a bit of a misnomer.  We only
looked at nodes with negative excess.  If we have an initialization scheme
that generates weak branches, and gives them labels based on their depth,
this value will be wrong.  However, calling [[isStrongNode]] on every node
would be prohibitive, $O(n^2)$.
<<Solver data>>=
    int maxWeakLabel;
    int maxInitialLabel;
<<default Solver constructor>>=
    maxWeakLabel = INIT_WEAK_LABEL;
    maxInitialLabel = INIT_WEAK_LABEL;
@ %def maxWeakLabel maxInitialLabel
\subsubsection{Blocking Path Initialization}
A simple extension to the idea of just simply saturating the
source- and sink-adjacent arcs is to try to push flow from the
source-adjacent nodes into the interior of the graph.  Our
objective is to try to build some simple but large components
with a minimum of work.

This is a bit more involved than it initially appeared.
The first thing to note is that the components we will build will
be paths.  This means that we can only push flow out of a node
along a single outgoing arc.  Also, we can only push flow into
a node from a single neighbor.  Otherwise, we might end up building
components that are not trees.

Because we wish to build components that are as large as possible
(rather than creating lots of small components), we want to avoid
splitting edges.  However, this can be problematic because many of
the random graphs we test with have more in-capacity than out-capacity
for the source-adjacent nodes.  If we blindly avoid splitting, we
won't move any flow from the source-adjacent nodes because they
have more excess than out-capacity.  

With all of this in mind, we proceed by finding nodes with 
positive excess that still have their initial label.
We then push the flow out of the node and expect that every node
we visit while pushing flow will be relabeled.  Basically, we
are using the node's label as a marker indicating that we have
visited the node.
<<Solver methods>>=
    void buildBlockingPathTree(LabelMethod labelMethod);
<<Solver method implementations>>=
void PhaseSolver::buildBlockingPathTree(LabelMethod labelMethod)
{
    saturateSourceSinkArcs();

    for (sourceNode->resetIterations();
        sourceNode->hasMoreNeighbors();
	sourceNode->advanceNeighbors())
    {
	Node& node = *sourceNode->getCurrentNeighbor().getOtherNode(sourceNode);
	if ((node.getExcess() > 0) && 
	    (node.getLabel() == Node::INITIAL_LABEL))
	{	
	    blockingPathPush(node);
	}
    }

    setLabelsAndStatus(labelMethod);
}
@ %def bulidBlockingPathTree
The real work in this greedy scheme is pushing the flow out of a node.
This recursive function finds the `best' edge to push the flow out on, and
then we push the flow.  When we push flow out, we make this node a child
of the node at the other end of the arc, and then we try to push flow out
of that node (hence the recursion).  As we visit each node, we relabel it
to prevent visiting the same node more than once.

If we reach a point where the flow cannot be pushed out of a node, that
node then becomes the root of a strong branch.  If we make it all the way
to a sink-adjacent node, and its deficit exceeds the excess we are
pushing, then it becomes a weak branch.
<<Solver methods>>=
    void blockingPathPush(Node& node);
<<Solver method implementations>>=
void PhaseSolver::blockingPathPush(Node& node)
{
    node.setLabel(INIT_STRONG_LABEL);

    NodeExcess excess = node.getExcess();
    Edge* bestOutEdge = nil;
    <<find best out-edge without split>>

    FlowAmount flow = excess;
    if (bestOutEdge != nil) {
	Node& otherNode = *bestOutEdge->getOtherNode(&node);
	TRACE( trout << "push positive excess (" << flow << " from " 
		     << node.getId() << " to " << otherNode.getId() << endl);
	         
	bestOutEdge->increaseFlow(flow);
	node.decrementExcess(flow);
	otherNode.incrementExcess(flow);
	otherNode.addChild(node, *bestOutEdge);

	<<recursively push flow out>>
    } else {
	TRACE( trout << "can't push excess from " << node.getId() << endl; )
    }
}
@ %def blockingPathPush
To find the best out edge, we iterate over all of the edges looking
for one that has at least enough capacity to handle the excess.  Also,
the edge must refer to a node that we haven't visited yet, as indicated
by its label.  If we find multiple edges with sufficient capacity, we
take the one with the least capacity.  Alternatively, we could take
the first fit, but we do not expect that scanning every edge will be
too burdensome.
<<find best out-edge without split>>=
    node.resetIterations();
    while (node.hasMoreNeighbors()) {
	Edge& edge = node.getCurrentNeighbor();
	if (edge.getOtherNode(&node)->getLabel() == Node::INITIAL_LABEL)
	{
	    if (edge.residCapacity(node) >= excess) {
		if (bestOutEdge == nil) {
		    bestOutEdge = &edge;
		} else if (bestOutEdge->residCapacity(node) > edge.residCapacity(node)) {
		    bestOutEdge = &edge;
		}
	    }
	}
	node.advanceNeighbors();
    }
    node.resetIterations();
@
Pushing flow out is very straightforward: just call
[[blockingPathPush(otherNode)]].  In the past it was more complicated
because we were setting the label while building the tree, and if we
reached the sink, then we had created a weak branch instead of a strong
one.  However, now we have separated the two steps and we simply build the
tree, pushing the excess as far as we can.  Then we look at the result and
apply labels as needed.
<<recursively push flow out>>=
    if (otherNode.getExcess() > 0) {	// ZERO_DEFICIT
	blockingPathPush(otherNode);
    }
@
\subsubsection{Splitting Path Initialization}
Greedy push with an arbitrary number of splits allowed.
<<Solver methods>>=
    void buildGreedyPathTree(LabelMethod labelMethod);
<<Solver method implementations>>=
void PhaseSolver::buildGreedyPathTree(LabelMethod labelMethod)
{
    saturateSourceSinkArcs();

    for (sourceNode->resetIterations();
        sourceNode->hasMoreNeighbors();
	sourceNode->advanceNeighbors())
    {
	Node& node = *sourceNode->getCurrentNeighbor().getOtherNode(sourceNode);
	if ((node.getExcess() > 0) && 
	    (node.getLabel() == Node::INITIAL_LABEL))
	{	
	    int numSplits = maxSplits;
	    splittingPathPush(node, numSplits);
	}
    }

    setLabelsAndStatus(labelMethod);
}
@ %def buildGreedyPathTree
<<public Solver data>>=
    int maxSplits;
@ %def maxSplits
The default maximum number of splits is arbitrarily set to ten.  It's
public so it can be easily changed by the driver program.
<<default Solver constructor>>=
    maxSplits = 10;
<<Solver methods>>=
    void splittingPathPush(Node& node, int& remainingSplits);
<<Solver method implementations>>=
void PhaseSolver::splittingPathPush(Node& node, int& remainingSplits)
{
    node.setLabel(INIT_STRONG_LABEL);
    NodeExcess excess = node.getExcess();
    EdgePtr bestOutEdge = nil;
    FlowAmount bestEdgeCap = MAXINT;
    <<find best out-edge with split>>

    if (bestOutEdge != nil) {
	Node& otherNode = *bestOutEdge->getOtherNode(&node);
	if ((bestEdgeCap < excess) && (remainingSplits <= 0)) {
	    return;
	}

	TRACE( trout << "push positive excess (" 
		     << ((excess < bestEdgeCap) ? excess : bestEdgeCap)
		     << ") from " << node.getId() << " to " 
		     << otherNode.getId() << endl);
	         
	otherNode.addChild(node, *bestOutEdge);
	Boolean split = node.pushToParent();
	if (split) {
	    remainingSplits--;
	}

	<<recursively push flow out 2>>

    } else {
	TRACE( trout << "can't push excess from " << node.getId() << endl; )
    }
}
@ %def blockingPathPush
To find the best edge, we iterate over all of the neighbors.  
We only consider nodes that haven't already been visited and 
had flow pushed through them (as indicated by the label).  To
be feasible and edge must have positive residual capacity.  Also,
if we cannot split any more, we need it to have capacity greater
than the excess.
<<find best out-edge with split>>=
    for (node.resetIterations();
	 node.hasMoreNeighbors();
	 node.advanceNeighbors())
    {
	Edge& edge = node.getCurrentNeighbor();
	Node& otherNode =  *edge.getOtherNode(&node);
	if (otherNode.getLabel() == Node::INITIAL_LABEL) {
	    FlowAmount edgeCap =  edge.residCapacity(node);
	    if ((edgeCap <= 0) ||
		((edgeCap <= excess) && (remainingSplits <= 0)))
	    {
		continue;
	    }

	    <<compare edge to best edge>>
	}
    }
@
If this is the first feasible edge we've seen, it must be the best.
<<compare edge to best edge>>=
    if (bestOutEdge == nil) {
	bestOutEdge = &edge;
	bestEdgeCap = edgeCap;
    } 
@  We're looking an edge with capacity greater than the excess
to avoid splits.
If there are multiple such edges, we'll choose the minimal one - i.e.
the edge with capacity closest to the excess.
<<compare edge to best edge>>=
    else if (edgeCap > excess) {
	if (bestEdgeCap > edgeCap) {
	    bestOutEdge = &edge;
	    bestEdgeCap = edgeCap;
	} else if (bestEdgeCap < excess) {
	    bestOutEdge = &edge;
	    bestEdgeCap = edgeCap;
	}
    } 
@ If we can't find an edge with capacity that exceeds the excess, 
we want to find one that the most capacity.  Again, looking for 
capacity closest to the excess.
<<compare edge to best edge>>=
    else if (edgeCap > bestEdgeCap) {
	bestOutEdge = &edge;
	bestEdgeCap = edgeCap;
    }
@ When we go to push flow out of [[otherNode]], we want to keep our
eyes open for reaching all the way to a sink-adjacent node.  If we
pushed flow to [[otherNode]] and its excess isn't positive, it must
be sink adjacent, so we can stop pushing.
<<recursively push flow out 2>>=
    if (otherNode.getExcess() > 0) {	
	splittingPathPush(otherNode, remainingSplits);
    } else {
	TRACE( trout << "found sink-adjacent node " << otherNode.getId() << endl;)
	CHECK_TREE(otherNode, nil);
    }
@ If we pushed flow out of this node, and it still has positive
excess, we must have split.  We just recursively call ourselves to
try to push again.  This will repeat the search of neighbors, but
we will be looking to push less excess this time, so the set of
feasible arcs will different.
<<recursively push flow out 2>>=
    if (node.getExcess() > 0) {
	splittingPathPush(node, remainingSplits);
    }
@
\subsubsection{Staturate All Arcs}
Another simple initialization scheme is to saturate all arcs in
the graph.  Then we classify each node as strong or weak
depending on whether it has positive or negative excess.
<<Solver methods>>=
    void saturateAllArcs(LabelMethod labelMethod);
<<Solver method implementations>>=
void PhaseSolver::saturateAllArcs(LabelMethod labelMethod)
{
    saturateSourceSinkArcs();

    for (int i = 0 ; i < numEdges; i++) {
	Edge& edge = edges[i];
	if ((edge.getDest() != sourceNode) &&
	    (edge.getSource() != sinkNode) &&
	    (edge.getFlow() == 0))
	{
	    FlowAmount excess = edge.saturate();

	    edge.getTail()->decrementExcess(excess);
	    edge.getHead()->incrementExcess(excess);
	}
    }

    setLabelsAndStatus(labelMethod);
}
@ %def saturateAllArcs
\subsection{Distance Labeling}
Taking a page from implementations of the push-relabel algorithm, we
have code to establish and maintain our labels based on distances to
the sink.  We can do this at initialization time and/or periodically
during the execution.
\subsubsection{Shortest Path Initialization}
To use distance labels during initialization, we start with
the usual staturated source and sink arcs.  Then, we perform a 
shortest-path search back from the sink linking all the zero-deficit
nodes into weak branches along the shortest paths, via a breadth-first
search.  We use the node label as a distance
label during this procedure.  When we are done, [[setSpInitialNodeStatus]]
will set the labels of the strong nodes based on the labels of their
weak neighbors.
<<Solver methods>>=
    void buildSpTree(LabelMethod labelMethod);
<<Solver method implementations>>=
void PhaseSolver::buildSpTree(LabelMethod labelMethod)
{
    saturateSourceSinkArcs();
    <<initialize all node labels>>
    resetQ();

    for(sinkNode->resetIterations();
        sinkNode->hasMoreNeighbors();
	sinkNode->advanceNeighbors())
    {
	<<load queue with weak roots>>
    }

    <<perform bfs labeling>>
    setSpInitialNodeStatus(labelMethod);	
    maxWeakLabel = maxInitialLabel;		// needed for highest label

    IFDEBUG( checkBranches(); );
}
@ %def buildSpTree
First, we need to set all the labels to infinity.
<<initialize all node labels>>=
    for (int i = 1; i <= numNodes; i++) {
	nodes[i].setLabel(MAXINT);
    }
@ Then, we iterate over each sink-adjacent node, ignoring the source node
if it's adjacent to the sink.
<<load queue with weak roots>>=
    Edge& edge = sinkNode->getCurrentNeighbor();
    Node& edgeSource = *edge.getSource();
    Node& edgeDest = *edge.getDest();
    if ((&edgeSource == sourceNode) || (&edgeDest != sinkNode) ||
	 (edgeSource.getExcess() >= 0))
    {
	continue;
    }
@ Once we're sure we a sink-adjacent node and it's not the source,
we label it, and start investigating its neighbors.
<<load queue with weak roots>>=
    edgeSource.setLabel(1);
    putNodeQ(edgeSource);

@ Once the queue is loaded with the sink-adjacent nodes, we just pull nodes
out of the queue until there are no more.
<<perform bfs labeling>>=
    for (NodePtr nodep = getNextNodeQ(); nodep != nil; nodep = getNextNodeQ()) {
	int dist = nodep->getLabel() + 1;
	ElIterator it = nodep->getNeighbors();
	for (EdgePtr edgep = it.getNext(); edgep != nil; edgep = it.getNext()) {
	    <<label weak neighbor as needed>>
	}
    }
@ If our neighbor has zero excess and we have residual capacity, then
we might be his shortest path to the sink.  If his label is greater than
our distance, it should be [[MAXINT]], and we are a shortest path for him.
Then, we become his parent, and we want to investigate all of his neighbors.

If his label is our distance, then we are not any closer, but may we have
more capacity.  In this case, we don't need to queue him to investigate his 
neighbors because he should already be in the queue from the time he got
labeled.
<<label weak neighbor as needed>>=
    Node& neighbor = *edgep->getOtherNode(nodep);
    FlowAmount residCap = edgep->residCapacity(neighbor);
    if ((neighbor.getExcess() == 0) && (residCap > 0)) {
	if (neighbor.getLabel() > dist) {
	    neighbor.setLabel(dist);
	    neighbor.distance = dist;
	    putNodeQ(neighbor);
	    nodep->addChild(neighbor, *edgep);
	} else if ((neighbor.getLabel() == dist) &&
		   (neighbor.getParentCapacity() < residCap))
	{
	    neighbor.split();
	    nodep->addChild(neighbor, *edgep);
	}
    }
@
Once we have built the weak trees, we need to establish the initial status
of all nodes.  If we want to set constant labels (one and two), we need to
reset the distances while we're at it.\footnote {This is due to the check
we currently have in [[processSubtree]] to check that the label is greater
than or equal to the distance.   After we experiment with integrated
distance labels, we may be able to remove that check and this code.}
Therefore, in addition to the usual function, [[setConstantLabels]], we
also reset the distances.

The other initial labeling schemes (distance to sink or deficit), are
equivalent, and we've already done the real work - computing the
distances.  The weak nodes are already correctly labeled, we just need to
set the label count appropriately.  For the strong nodes, we need to find
the minimum label of its weak neighbors, and we also need to add it as a
strong branch.
<<Solver methods>>=
    void setSpInitialNodeStatus(LabelMethod labelMethod);
<<Solver method implementations>>=
void PhaseSolver::setSpInitialNodeStatus(LabelMethod labelMethod)
{
    switch(labelMethod) {
    case LABELS_CONSTANT:
	setConstantLabels();
	setInitialLabelCounts();
	for (int i = 1; i <= numNodes; i++) {
	    Node& node = nodes[i];
	    node.distance = node.getLabel();
	}
	break;
    default:
	maxInitialLabel = INIT_WEAK_LABEL;
	for (int i = 1; i <= numNodes; i++) {
	    Node& node = nodes[i];
	    if (node.getExcess() > 0) {
		<<set label to minimum of weak neighbors>>
		addStrongBranch(node);
	    }  else if (node.getLabel() == MAXINT) {
		// notice with cher graphs that some zero-deficit nodes are
		// unreachable during initialization???
		node.setLabel(INIT_WEAK_LABEL);
	    }
	    labelCount[node.getLabel()]++;
	    node.resetIterations();
	    if (node.getLabel() > maxInitialLabel) {
		maxInitialLabel = node.getLabel();
	    }
	}
    }
}
@ %def setSpInitialNodeStatus 
For a strong node, we iterate over its neighbors looking for nodes with
non-positive excess and find the minimum label.  
{\em We don't really care about in/out arcs, but source/sink node could
still be a problem?}
<<set label to minimum of weak neighbors>>=
    NodeLabel minLabel = MAXINT;
    for (node.resetIterations();
	 node.hasMoreNeighbors();
	 node.advanceNeighbors())
    {
	Edge& neighborEdge = node.getCurrentNeighbor();
	Node& neighbor = *neighborEdge.getOtherNode(&node);
	if (neighbor.getExcess() <= 0) {
	    if (minLabel > neighbor.getLabel()) {
		minLabel = neighbor.getLabel();
	    }
	}
    }
@
We then set the label to the minimum plus one.
There is one catch that we noticed with Cheriyan graphs: all
the neighbors could be unreachable from the sink, so they'd have
MAXINT labels.  In that case, we just use the standard initial
label for strong nodes.
<<set label to minimum of weak neighbors>>=
    if (minLabel < MAXINT) {
	assert(minLabel < numNodes);
	node.setLabel(minLabel + 1);
	node.distance = minLabel + 1;
    } else {
	node.setLabel(INIT_STRONG_LABEL);
    }
@
\subsubsection{Global Relabeling Heuristic}
During the execution of the algorithm, regardless of the initialization
method used we can periodically relabel nodes based on their distances to
the sink in the residual graph.   Setting the labels this way `guides' the
excess from the strong nodes toward the sink node.\footnote {Note that
because the saturate-all method may create deficit nodes that are not
adjacent to the sink, we actually set the distances to the nearest deficit
node.  Therefore, we are directing flow towards deficit nodes rather than
the sink.}  Because global relabeling is a rather expensive operation, we
don't want to perform it too often.

We cannot necessarily blindly set the
labels to their distances because we wish to preserve 
monotonicity within a branch.  While labeling nodes,
if we discover that a strong branch cannot reach the sink, we can
eliminate it from further processing.
<<Solver methods>>=
    void globalRelabel();
@ Before getting into the implementation of the the relabeling function, 
let's define
some small functions that we call from the solver loop to perform
the global relabeling.  Global relabeling is invoked based on the number of
relabels that the algorithm has (normally) performed.
So, we will maintain a counter that we compare to the number relabels 
to tell us when to trigger the global relabel.
<<Solver data>>=
    int relabelCounter;
@ %def relabelCounter
This is then set and updated as a factor of the number of nodes in the graph.
A non-positive value means no relabeling.
<<public Solver data>>=
    float relabelFrequency;
<<default Solver constructor>>=
    relabelFrequency = 0.0;
@ %def relabelFrequency
Now, we can check the counter, and if needed perform the relabeling
and `schedule' the next relabel.
<<Solver methods>>=
    void checkForRelabel();
<<Solver inline implementations>>= 
INLINE void PhaseSolver::checkForRelabel()
{
    if (numRelabels > relabelCounter) {
	globalRelabel();
	relabelCounter = numRelabels + (int)(numNodes * relabelFrequency);
    }
}
@ %def checkForRelabel
Then, we just need to schedule the first relabel operation.  If the frequency
is zero, then we set the counter to MAXINT to prevent relabeling from
ever happening.
<<Solver methods>>=
    void initGlobalRelabel();
<<Solver inline implementations>>= 
INLINE void PhaseSolver::initGlobalRelabel()
{
    if (relabelFrequency > 0.0) {
	relabelCounter = numRelabels + (int)(numNodes * relabelFrequency);
    } else {
	relabelCounter = MAXINT;
    }
}
@ %def initGlobalRelabel
Now we can present the relabeling function, which is conceptually rather
simple.  We initialize all the distances, then perform a BFS to set the
distances to their new value.  Then we try to set the labels to match
their distances, respecting monotonicity.
<<Solver method implementations>>=
void PhaseSolver::globalRelabel()
{
    STATS(numGlobalRelabels++);
    TRACE( trout << "Begin global relabeling.  Lowest label: " << lowestLabel << endl; );
    Boolean pruningEnabled = TRUE;
    <<set initial distance labels>>
#ifdef SINK_DIST_RELABEL
    <<load the bfs queue with sink-adj nodes without labeling>>
#endif /* SINK_DIST_RELABEL */
    <<search backwards from the sink labeling nodes>>
    <<relabel/remove each branch>>
    IFDEBUG( checkBranches(); )
}
@ %def globalRelabel
We start by labeling most nodes with an initial distance of [[MAXINT]].
The exception is nodes with negative excess.  Normally, these would
be adjacent to the sink, but with saturate-all, they can be anywere
in the graph.  Because they are weak roots, we will label them
with a distance of one, even though they are not adjacent to
the sink.  
<<set initial distance labels>>=
    resetQ();
    for (int i = 1; i <= numNodes; i++) {
	Node& node = nodes[i];
#ifdef SINK_DIST_RELABEL
	node.distance = MAXINT;
#else
	if (node.getExcess() < 0) {
	    node.distance = 1;
	    putNodeQ(node);
	    // if (node.getLabel() > INIT_WEAK_LABEL) - disable pruning
	} else {
	    node.distance = MAXINT;
	}
#endif /* SINK_DIST_RELABEL */
    }
<<load the bfs queue with sink-adj nodes without labeling>>=
    for(sinkNode->resetIterations();
        sinkNode->hasMoreNeighbors();
	sinkNode->advanceNeighbors())
    {
	Edge& edge = sinkNode->getCurrentNeighbor();
	Node& edgeSource = *edge.getSource();
	Node& edgeDest = *edge.getDest();
	if ((&edgeSource != sourceNode) && (&edgeDest == sinkNode))

	{
	    edgeSource.distance = 1;
	    putNodeQ(edgeSource);
	}
    }
@ Once we've labeled the nodes and put the negative excess ones in the queue,
we processing nodes in the queue until done.  This performs
a breadth-first search.
<<search backwards from the sink labeling nodes>>=
    for (NodePtr nodep = getNextNodeQ(); nodep != nil; nodep = getNextNodeQ()) {
	int dist = nodep->distance + 1;
	ElIterator it = nodep->getNeighbors();
	for (EdgePtr edge = it.getNext(); edge != nil; edge=it.getNext()) {
	    <<label neighbor as needed>>
	}
    }
@ If this is the first time we've visited this node, it's distance label 
will still be [[MAXINT]].  In that case, we check if we have residual
capacity from the neighbor to ourselves.  If so, we label the node and
put it in the queue to eventually process its neighbors.  If the neighbor's
distances is less than [[MAXINT]] we must have already visited it, and its
distance is less than or equal to our current one.  So, we are done with it.
<<label neighbor as needed>>=
    Node& neighbor = *edge->getOtherNode(nodep);
    if (neighbor.distance == MAXINT) {
    //if (neighbor.distance > dist) {	// allow visting node multiple times - fix comment above
	FlowAmount residCap = edge->residCapacity(neighbor);

	if (residCap > 0) {
	    neighbor.distance = dist;
	    putNodeQ(neighbor);
	}
    }
@
After we've labeled the nodes, we want to scan all of the branches.  For
strong branches, we would like to remove them or update their labels to
reflect their distances.  For weak branches, we will just update the
labels.  However, with saturate-all initialization, we have to beware of
nodes that (temporarily) have infinite distance labels due to the `wacky'
state that saturating all arcs creates (e.g. we cannot even perform flow
recovery immediately after saturating all arcs).

To do this relabeling, we must iterate over all nodes in the graph looking 
for roots because we don't track weak branches.  
{\em Tracking weak branches wouldn't be too much work because we never create
any new ones.  If we had a doubly-linked list it would be pretty simple.}
<<relabel/remove each branch>>=
    for (int i = 1; i <= numNodes; i++) {
	Node& node = nodes[i];
	if ((&node == sourceNode) || (&node == sinkNode)) {
	    continue;
	}
	if (node.isRootNode()) {
	    int distance = node.distance;
	    if (isStrongNode(node)) {
		<<relabel/remove strong branch>>
	    } else {
		if (distance < numNodes) {
		    relabelBranch(node);
		}
	    }
	}
    }
@  If the root of a strong branch has node label of [[numNodes]],
it has already been removed from the graph (by this code).  Otherwise if
it has a distance label of MAXINT, we assume
the whole branch does too, and we can remove it from the bucket.
We set the labels of all nodes in the branches to [[numNodes]] to
prevent any strong branches from trying to merge with this branch.
The exception is if we have disabled pruning.  In which case we ignore
the branch, leaving the labels as they are and the branch in the bucket.
<<relabel/remove strong branch>>=
    NodeLabel oldLabel = node.getLabel();
    if (oldLabel < numNodes) {  
	// we haven't already pruned this branch
	if ((distance == MAXINT) && pruningEnabled) {
	    buckets[oldLabel].removeNode(node);
	    setBranchLabel(node, numNodes);
	} else {
	    <<relabel strong branch>>
	}
    } 
@ If the label is less than MAXINT, we can relabel it.
If we change the label of the root, we need to move it to another
bucket.
<<relabel strong branch>>=
    NodeLabel newLabel = relabelBranch(node);
    if (newLabel != oldLabel) {
	assert(newLabel > oldLabel);
	buckets[oldLabel].removeNode(node);
	addStrongBranch(node);
    }
@ This is a straightfoward, recursive DFS of the children
to set the labels of all nodes
of a branch to a specified value.
<<Solver methods>>=
    void setBranchLabel(Node& node, NodeLabel l);
<<Solver method implementations>>=
void PhaseSolver::setBranchLabel(Node& node, NodeLabel l)
{
    TRACE( trout << "relabeling " << node.getId() << " from " 
			 << node.getLabel() << " to " << l << endl; );
    setLabel(node, l);
    ElIterator it = node.getChildren();
    for (EdgePtr edge = it.getNext(); edge != nil; edge=it.getNext()) {
	setBranchLabel(*edge->getOtherNode(&node), l);
    }
}
@ %def setBranchLabel
To relabel a branch, we need to respect the 
monotonicity in the tree.  In a weak branch, monotonicity doesn't
matter so long as the branch stays weak, but if it later becomes strong,
we must have monotonicity for the rest of the implementation to work.
Ideally, we'd like to increase our label to match our distance.  However,
our new label cannot be higher than any of our children.  So, first
we recursively set the labels of our children and in the process compute
the minimum label among them.  Then we update our own label.
{\em we should profile and see if this should be an iterative DFS by
converting the queue to a stack.}
<<Solver methods>>=
    NodeLabel relabelBranch(Node& node);
<<Solver method implementations>>=
NodeLabel PhaseSolver::relabelBranch(Node& node)
{
    <<skip infinite labels>>
    NodeLabel minLabel = MAXINT;
    ElIterator it = node.getChildren();
    for (EdgePtr edge = it.getNext(); edge != nil; edge=it.getNext()) {
	NodeLabel l = relabelBranch(*edge->getOtherNode(&node));
	if (l < minLabel) {
	    minLabel = l;
	}
    }

    if (minLabel == MAXINT) {
	// node has no children
	minLabel = max(node.distance, node.getLabel());
    }
    assert(minLabel >= node.getLabel());
    node.minChildLabel = minLabel;

    if (node.distance > node.getLabel()) {
	NodeLabel newLabel = min(minLabel, node.distance);
	assert(newLabel >= node.getLabel());

	if (newLabel > node.getLabel()) {
	    TRACE( trout << "relabeling " << node.getId() << " from " 
			 << node.getLabel() << " to " << minLabel << endl; );
	    setLabel(node, newLabel);
	}
    }

    return node.getLabel();
}
@ %def relabelBranch
If we do not split when an arc goes to zero residual capacity during 
renormalization ([[splitOnZeroCapacity=FALSE]]), then we can have nodes in
a branch whose distance label is [[MAXINT]] because their parent edge has
no residual capacity (and their are no other paths to the node).  
In this case, we do not relabel the node.
<<skip infinite labels>>=
    if (node.distance > numNodes) {
	cout << "Not relabeling " << node.getId() << " due to infinite distance" << endl;
	return node.getLabel();
    }
@
After scanning a node's neighbors and children, if we found no merger
we need to increase the label by at least one.
If we are using distance labels, it is possible to increase the node label
by more, perhaps up to the distance label.  However, we cannot increase it
higher than any of our children because that would violate monotonicity.

One other issue to be aware of is that sometimes we are asked to increase
the label of a node with infinite distance.  This can happen during global
relabeling in two known cases.  First, if we don't split an arc with zero
residual capacity, the top of a branch may have a path to the sink (and
hence finite distance) while a sub-tree below a zero-capacity arc has no
path.  Secondly, with saturate-all initialization, we can have nodes with
no path to the sink, and hence infinite label.  Therefore, if we encounter
a node with infinite distance, we just increment the label rather than
increasing it up to its distance.
<<Solver methods>>=
    void increaseNodeLabel(Node& node);
<<Solver inline implementations>>= 
INLINE void PhaseSolver::increaseNodeLabel(Node& node)
{
    if ((node.distance > 0) && (node.distance < numNodes)) {
	NodeLabel oldLabel = node.getLabel();
	NodeLabel newLabel = min(node.distance, node.minChildLabel);
	newLabel = max(newLabel, oldLabel + 1);
	setLabel(node, newLabel);
	if (node.distance < newLabel) {
	    node.distance = newLabel;
	}
    } else {
	incrementLabel(node);
	STATS(numRelabels++);
    }
    CHECK_TREE(node, node.getParentEdge()); 	// check monotonicity
}
@ %def increaseNodeLabel
This labeling method sets the label to the distance from the node to the
sink in the original (not residual) graph.  Unfortunately, this is a lot
of cut-and-paste from the global relabeling code.
<<Solver methods>>=
    void setSinkDistLabels();
<<Solver method implementations>>=
void PhaseSolver::setSinkDistLabels()
{
    resetQ();
    <<initialize distances>>
    <<load the bfs queue with sink-adj nodes>>
    <<perform bfs distance labeling in orig graph>>
    //setSpInitialNodeStatus();	
    <<scan nodes and put in buckets>>

    IFDEBUG( checkBranches(); );
}
@ %def setSinkDistLabels
First, we need to set all the labels to infinity.
<<initialize distances>>=
    for (int i = 1; i <= numNodes; i++) {
	nodes[i].distance = MAXINT;
	nodes[i].setLabel(INIT_WEAK_LABEL);
    }
<<load the bfs queue with sink-adj nodes>>=
    for(sinkNode->resetIterations();
        sinkNode->hasMoreNeighbors();
	sinkNode->advanceNeighbors())
    {
	Edge& edge = sinkNode->getCurrentNeighbor();
	Node& edgeSource = *edge.getSource();
	Node& edgeDest = *edge.getDest();
	if ((&edgeSource != sourceNode) && (&edgeDest == sinkNode))

	{
	    edgeSource.distance = 1;
	    edgeSource.setLabel(1);
	    putNodeQ(edgeSource);
	}
    }
<<perform bfs distance labeling in orig graph>>=
    for (NodePtr nodep = getNextNodeQ(); nodep != nil; nodep = getNextNodeQ()) {
	int dist = nodep->distance + 1;
	ElIterator it = nodep->getNeighbors();
	for (EdgePtr edgep = it.getNext(); edgep != nil; edgep = it.getNext()) {
	    <<set neighbor distance as needed>>
	}
    }
<<set neighbor distance as needed>>=
    Node& neighbor = *edgep->getOtherNode(nodep);
    if ((&neighbor != sourceNode) && (&neighbor != sinkNode) &&
	(edgep->getDest() == nodep) &&		// want an 'in' arc
	(neighbor.distance > dist))
    {
	    neighbor.distance = dist;
	    // neighbor.setLabel(dist);  - using relabelBranch
	    putNodeQ(neighbor);
    }

@
This function sets distance labels to the nearest negative deficit node,
and it sets the node labels to the highest value possible while still
observing monotonicity.  Then, it puts the strong nodes in buckets.
<<Solver methods>>=
    void setDeficitDistLabels();
<<Solver method implementations>>=
void PhaseSolver::setDeficitDistLabels()
{
    resetQ();
    <<initialize distances and load queue>>
    <<perform bfs distance labeling>>
    <<scan nodes and put in buckets>>

    IFDEBUG( checkBranches(); );
}
@ %def setDeficitDistLabels
We establish initial distances and labels.  All nodes get an initial label
of one ([[INIT_WEAK_LABEL]]).  Deficit nodes get a distance of one, and they
get inserted into the BFS queue.  All other nodes get an infinite distance.
<<initialize distances and load queue>>=
    for (int i = 1; i <= numNodes; i++) {
	Node& node = nodes[i];
	node.setLabel(INIT_WEAK_LABEL);
	if (node.getExcess() < 0) {
	    node.distance = 1;
	    putNodeQ(node);
	} else {
	    node.distance = MAXINT;
	}
    }
<<initialize distances and load queue>>=
    // maybe I should scan the sink-adj nodes looking for
    // zero-defiict nodes

@ The BFS is simple: pull a node out of the queue and scan its neighbors.
Continue until there are no nodes in the queue.
<<perform bfs distance labeling>>=
    for (NodePtr nodep = getNextNodeQ(); nodep != nil; nodep = getNextNodeQ()) {
	int dist = nodep->distance + 1;
	ElIterator it = nodep->getNeighbors();
	for (EdgePtr edgep = it.getNext(); edgep != nil; edgep = it.getNext()) {
	    <<set zero-deficit neighbor distance as needed>>
	}
    }
@ When we scan a node, we are looking for neighbors 
that can reach us in the residual graph - i.e. positive residual capacity
from the neighbor to us.  We are only interested in neighbors
with a distance greater than one more than our distance (due to BFS, 
the distance should always be infinity).   If we find such a node,
we set its distance and label, and put it at the end of the queue
for subsequence processing.
<<set zero-deficit neighbor distance as needed>>=
    Node& neighbor = *edgep->getOtherNode(nodep);
    if ((neighbor.distance > dist) && 
//	(edgep->getDest() == nodep)) 		// want an 'in' arc
    (edgep->residCapacity(neighbor) > 0))	// hack - no resid cap check
    {
	    neighbor.distance = dist;
	    putNodeQ(neighbor);
    }
@ After we have performed the BFS, the labels are all initialized to
either their distance or the default value of one, and the distances
reflect the distance to a deficit node, which may still be infinity.
For simplicity, let's assume that this code is only called by
[[saturateAll]] so we have no branch structure to worry about - we
only worry about nodes.  

If an excess node has infinite distance, it cannont reach a weak node,
so we can ignore it (by setting its label to [[numNodes]] and not
putting it in a bucket).  Other strong nodes go into buckets, and
any other nodes are initialized and ready to go.  For all nodes,
we update the label count appropriately.
<<scan nodes and put in buckets>>=
    maxInitialLabel = INIT_WEAK_LABEL;
    for (int i = 1; i <= numNodes; i++) {
	Node& node = nodes[i];
	if ((&node == sourceNode) || (&node == sinkNode)) {
	    continue;
	}
	if (node.isRootNode() && (node.distance < MAXINT)) {
	    relabelBranch(node);
	}
	if (node.getExcess() > 0) {
	    if (node.distance == MAXINT) {
		setBranchLabel(node, numNodes);
		continue;
	    } else {
		addStrongBranch(node);
	    }
	}  else if (node.distance == MAXINT) {
	    // notice with cher graphs that some zero-deficit nodes are
	    // unreachable during initialization???
	    // XXX this might be an uneeded hack now
	    node.setLabel(INIT_WEAK_LABEL);
	    node.distance = 1;
	}
	labelCount[node.getLabel()]++;
	node.resetIterations();
	if (node.getLabel() > maxInitialLabel) {
	    maxInitialLabel = node.getLabel();
	}
    }
@
\subsubsection{Queue of Nodes}
Here is a queue of nodes.  This ought to be moved to Node.nw.
<<Solver data>>=
    NodePtr* nodeQ;
    NodePtr* qRead;
    NodePtr* qWrite;
<<Solver methods>>=
    void resetQ();
    NodePtr getNextNodeQ();
    void putNodeQ(Node& node);
<<Solver inline implementations>>= 
INLINE void PhaseSolver::resetQ()
{
    qRead = qWrite = nodeQ;
    *qRead = nil;
}
@ %def resetQ
<<Solver inline implementations>>= 
INLINE NodePtr PhaseSolver::getNextNodeQ()
{
    if (qRead < qWrite) {
	return *qRead++;
    } else {
	return nil;
    }
}
@ %def getNextNodeQ
<<Solver inline implementations>>= 
INLINE void PhaseSolver::putNodeQ(Node& node)
{
    assert(qWrite <= (nodeQ + numNodes + 1));
    *qWrite = &node;
    qWrite++;
}
@ %def putNodeQ
\subsection{Recovering a Feasible Flow}
When the pseudoflow algorithm completes, there are no merger arcs between
the strong nodes and weak nodes, and there is excess at the roots of
strong branches and deficits at the roots of weak branches.  We need
to return the excess/deficit to the source/sink.  We do this by
reducing the flows on arcs along paths to the source/sink.

To find paths to the source/sink we start from nodes with excess/deficit
and perform a depth first search along neighbor arcs that have what
we call 'reduction capacity', which is like residual capacity except
that it only applies to reducing the flow on arcs.  When we find
the source/sink, we reduce the flow along the path we identified by
an amount equal to the capacity of the bottleneck arc.  While we search,
we label each node with a mark value to identify cycles.

This code is not really a bottleneck, so we don't need to optimize it
much.  However, if we were to scan the neighbors of the source and
sink node, we could easily return flow in one simple push.  This works
really well for the sink because (unless were are using saturate-all)
all of the deficit nodes are adjacent to the sink.
<<Solver public declarations>>=
    void convertToFlow();
<<Solver method implementations>>=
void PhaseSolver::convertToFlow()
{
    currentMark = 1024;
    <<replace source and sink arcs>>
    returnSinkAdjDeficit();
    // returnSourceAdjExcess();		XXX - broken!?!

    for (int i = 1; i <= numNodes; i++) {
	Node& node = nodes[i];
	if (&node == sourceNode || &node == sinkNode) {
	    continue;
	}
	if (node.getExcess() != 0) {
	    pushRootExcess(node);
	}
    }
}
@ %def convertToFlow
<<Solver data>>=
    int currentMark;
@
When we initialized the psuedoflow (in [[saturateSourceSinkArcs]]), 
we removed the arcs from/to the
source/sink to prevent flow from going back to the source/sink.
Now, we actaully want to be able to get to the source and sink,
so we add them back.  We begin by replacing arcs that go from the
sink to a source-adjacent node in the source-adjacent node.
<<replace source and sink arcs>>=
    for(sourceNode->resetIterations();
        sourceNode->hasMoreNeighbors();
	sourceNode->advanceNeighbors())
    {
	Edge& edge = sourceNode->getCurrentNeighbor();
	Node& edgeSource = *edge.getSource();
	Node& edgeDest = *edge.getDest();
	if ((&edgeSource == sourceNode) && (&edgeDest != sinkNode)) {
	    edgeDest.addNeighbor(edge);
	} 
    }
@ Replace arcs that go from a sink-adjacent node in to the sink in the
sink-adjacent node.
<<replace source and sink arcs>>=
    for(sinkNode->resetIterations();
        sinkNode->hasMoreNeighbors();
	sinkNode->advanceNeighbors())
    {
	Edge& edge = sinkNode->getCurrentNeighbor();
	Node& edgeSource = *edge.getSource();
	Node& edgeDest = *edge.getDest();
	if ((&edgeSource != sourceNode) && (&edgeDest == sinkNode)) {
	    edgeSource.addNeighbor(edge);
	} 
    }
@
For initializations schemes other than saturate-all, we know that
all of the weak roots will be adjacent to the sink.  This makes
returning their deficit trivial - just one arc, and it should always
have sufficient capacity.
<<Solver public declarations>>=
    void returnSinkAdjDeficit();
<<Solver method implementations>>=
void PhaseSolver::returnSinkAdjDeficit()
{
    for(sinkNode->resetIterations();
        sinkNode->hasMoreNeighbors();
	sinkNode->advanceNeighbors())
    {
	Edge& edge = sinkNode->getCurrentNeighbor();
	Node& sinkAdjNode = *edge.getSource();
	if (&sinkAdjNode != sinkNode) {
	    FlowAmount excess = sinkAdjNode.getExcess();
	    if (excess < 0) {
		FlowAmount amt = -excess;
		if (amt > edge.getFlow()) amt = edge.getFlow();
		edge.reduceFlow(amt);
		sinkAdjNode.incrementExcess(amt);
		TRACE( trout << "returned " << -excess << " units from "
		             << sinkAdjNode.getId() << " to sink" << endl; );
	    }
	}
    }
}
@ %def returnSinkAdjDeficit
For strong nodes, we are not so lucky.  However, we can still pick
up some `low hanging fruit'.  Chances are there is no excess at a
source-adjacent node because it has merged with weak nodes.  However,
we can trace the path up to the root of a source-adjacent node.
We can return the excess at the root to the source via the path we
navigated to find the root.  This will remove excess from some
strong nodes in the graph but not all of them because our root may
have merged and split.  This will be more efficient that discovering
the path from the root node.
<<Solver public declarations>>=
    void returnSourceAdjExcess();
<<Solver method implementations>>=
void PhaseSolver::returnSourceAdjExcess()
{
    for(sourceNode->resetIterations();
        sourceNode->hasMoreNeighbors();
	sourceNode->advanceNeighbors())
    {
	Edge& edge = sourceNode->getCurrentNeighbor();
	Node& srcAdjNode = *edge.getDest();
	FlowAmount bottleCap = edge.getFlow();
	<<find bottleneck capacity to strong root>>
	if (bottleCap > 0) {
	    <<reduce flow along path to strong root>>
	}
    }
}
@ %def returnSourceAdjExcess
First we need to find the bottleneck capacity of all arcs along
the path to the root of the branch.  We do this iteratively rather
than recursively.  Theoretically, this should yeild better performance.
<<find bottleneck capacity to strong root>>=
    Node* currNode = &srcAdjNode;
    while (currNode->isRootNode() == FALSE) {
	FlowAmount parentCap = currNode->getParentEdge()->getFlow();
	if (parentCap < bottleCap) {
	    bottleCap = parentCap;
	}
	currNode = currNode->getParentNode();
    }
@ Then we need to take the minimum of that and excess of the root.  If the 
root has a deficit, we indicate that by setting the bottleneck capacity
to zero.
<<find bottleneck capacity to strong root>>=
    FlowAmount branchExcess = currNode->getExcess();
    if (branchExcess > 0) {
	if (branchExcess < bottleCap) {
	    bottleCap = branchExcess;
	}
    } else {
	bottleCap = 0;
    }
@ Once we know the bottleneck capacity, reducing the flow is pretty 
trivial starting from the arc out of the source.  Again we do
this iteratively.
<<reduce flow along path to strong root>>=
    edge.reduceFlow(bottleCap);
    currNode = &srcAdjNode;
    while (currNode->isRootNode() == FALSE) {
	currNode->getParentEdge()->reduceFlow(bottleCap);
	currNode = currNode->getParentNode();
    }
    currNode->decrementExcess(bottleCap);
    TRACE( trout << "returned " << bottleCap << " units from "
                 << srcAdjNode.getId() << " (" 
		 << srcAdjNode.getExcess() << ") to source" << endl; );
@
If a node has excess/deficit, we begin search from the node among its
neighbors trying to find a path back to the source/sink.
Each time we find a path, we reduce the excess/deficit on this node.
We continue until the node has zero excess.
<<Solver public declarations>>=
    void pushRootExcess(Node& node);
<<Solver method implementations>>=
void PhaseSolver::pushRootExcess(Node& node)
{
    Boolean seekingSource = (node.getExcess() > 0) ? TRUE : FALSE;
    while (node.getExcess() != 0) {
	currentMark++;
	FlowAmount bottleCap = (seekingSource) ? 
	                        node.getExcess() : -node.getExcess();

	Boolean found = visitNode(node, node, bottleCap, 
	                                seekingSource);
	if (found) {
	    TRACE( trout << "Pushing " << bottleCap 
	                 << " units of excess from " << node.getId() 
			 << " (" << node.getExcess() << ")" << endl;)
	    if (seekingSource) { 
		node.decrementExcess(bottleCap);
		assert(node.getExcess() >= 0);
	    } else {
		node.incrementExcess(bottleCap);
		assert(node.getExcess() <= 0);
	    }
	} else {
	    //assert("There must be a path to the source/sink" == nil);
	    cout << "There must be a path to the source/sink " << node.getId() << endl;
	    return;
	}

	if (node.getExcess() != 0) {
	    assert(node.hasMoreNeighbors());
	}
    }
}
@ %def pushRootExcess
To dispose of the excess/deficit at a node, we recursively search
its neighbors looking for the source/sink.  Once we find the source/sink,
we reduce the flow along the path from the node to the source/sink
by the amount of the bottleneck capacity along the path.  We ignore
arcs with zero residual capacity.

The other possible outcome of our search is that we find a cycle.
Hoch97 says
we want to reduce the flow of all arcs along the cycle.
However, we have found that this is unnecessary.  If we find
a node that we've already visited, we just ignore it and search
for other paths to the source/sink.  Theoretically, ours is an
$O(m^2)$ algorithm compared to $O(mn)$.  However, we found no
appreciable difference in practice.  The basic difference is that
cycle-cancelling expends effort reducing the flow along cycles
which does not reduce the excess/deficit, which is a bit of a waste
of effort.  Whereas our algorithm does not waste the effort, but may
expend more effort because it could continue `stumbling' into the
same cycles over and over.
<<Solver public declarations>>=
    Boolean visitNode(Node& node, Node& prevNode, FlowAmount& bottleCap, 
    		  Boolean seekingSource);
<<Solver method implementations>>=
Boolean
PhaseSolver::visitNode(Node& node, Node& prevNode, FlowAmount& bottleCap, 
                       Boolean seekingSource)
{
    <<check for terminating recursion>>
    Boolean result = FALSE;
    node.flag = currentMark;

    node.resetIterations();
    while (node.hasMoreNeighbors()) {
	Edge& neighborEdge = node.getCurrentNeighbor();
	Node& neighbor = *neighborEdge.getOtherNode(&node);
	FlowAmount residCap = neighborEdge.reductionCapacity(
				    seekingSource ? neighbor : node);
	if (residCap > 0) {
	    if (&neighbor != &prevNode) {
		<<visit neighbor node>>
	    }
	} else {
	    // useless arc - remove it
	    node.removeNeighbor(neighborEdge);
	}
	node.advanceNeighbors();
    }

    return result;
}
@ %def visitNode
We terminate the recursion when we find the source/sink.
We could have eliminated the tail recursion by checking for it
before visiting the node (in the [[while]] loop), but this seems
simpler.  
Note, it should be impossible to find a path to the sink/source if we
are searching for the source/sink.
<<check for terminating recursion>>=
    if (node.flag == currentMark) {
	return FALSE;
    } else if (&node == sourceNode) {
	assert(seekingSource);
	return TRUE;
    } else if (&node == sinkNode) {
	assert(!seekingSource);
	return TRUE;
    }
@
To visit a node, we compute a new, tentative bottleneck capacity, but
we do not assign it to the bottleneck parameter (which is passed by
reference) until we find the source/sink.  After visiting
the node (and its neighbors), if we found something, we will
reduce the flow on the arc, and break the search to return.
to the calling function.
<<visit neighbor node>>=
    FlowAmount newBottleCap = (bottleCap < residCap) ? bottleCap : residCap;
    Boolean status = visitNode(neighbor, node, newBottleCap, 
                                    seekingSource);
    // see if we found source/sink or a cycle
    if (status) {
	bottleCap = newBottleCap;
	reduceFlow(node, neighborEdge, bottleCap);
	result = status;
	break;		// stop checking neighbors
    } 
@ 
When we reduce the flow on an edge, if the edge becomes saturated,
we need to remove it from the appropriate node.  Currently, this
code is `unidirectional' (i.e. we ignore the orientation of the arc),
so we express everything in terms of the from/to direction we
are moving flow along.
<<Solver public declarations>>=
    void reduceFlow(Node& from, Edge& edge, 
		    FlowAmount amt);
<<Solver method implementations>>=
void PhaseSolver::reduceFlow(Node& from, Edge& edge, FlowAmount amt)
{
    edge.reduceFlow(amt);

    if (edge.getFlow() == 0) {
	from.removeNeighbor(edge);
    }
}
@ %def reduceFlow

@
\subsection{Parametric Sensitivity Analysis}
We support simple parametric sensitivity analysis based on the form
used by GGT.  Given a sequence of increasing values of a parameter, 
$\lambda$, the source-adjacent arcs have non-decreasing capacity, and
the sink-adjacent arcs have non-increasing capacity.  

We establish the initial capacity of the network by setting an
initial value for $\lambda$ and solve the resulting network.  Then,
we can set new values of $\lambda$, update the capacities, and re-solve
the network in less time than starting from scratch.

Setting the initial capacity is simple: just set the parameter for
the source- and sink-adjacent arcs.
<<Solver public declarations>>=
    void setInitialCapacity(double lambda);
<<Solver method implementations>>=
void PhaseSolver::setInitialCapacity(double lambda)
{
    for(sourceNode->resetIterations();
        sourceNode->hasMoreNeighbors();
	sourceNode->advanceNeighbors())
    {
	Edge& edge = sourceNode->getCurrentNeighbor();
	Node& otherNode = *edge.getDest();
	if (&otherNode != sourceNode) {
	    Boolean negative = edge.setInitialCapacity(lambda);
	    if (negative) {
		sourceNode->removeNeighbor(edge);
		edge.reInit(otherNode, *sinkNode);
		sinkNode->addNeighbor(edge);
	    }
	}
    }

    for(sinkNode->resetIterations();
        sinkNode->hasMoreNeighbors();
	sinkNode->advanceNeighbors())
    {
	Edge& edge = sinkNode->getCurrentNeighbor();
	Node& otherNode = *edge.getSource();
	if (&otherNode != sinkNode) {
	    Boolean negative = edge.setInitialCapacity(lambda);
	    if (negative == FALSE) {
		sinkNode->removeNeighbor(edge);
		edge.reInit(*sourceNode, otherNode);
		sourceNode->addNeighbor(edge);
	    }
	}
    }
}
@ %def setInitialCapacity
Updating the capacity to the next parameter value is a bit more involved.
Adjusting the capacities once a flow has been established may create
excess at the the source- or sink-adjacent node.  This excess needs to be
pushed towards the root of the branch containing the node.  This may
create a new strong branch if the amount of excess exceeds the previous
deficit at a weak root.  For strong branches, it will only increase the
excess at the root.  Because of this, we do not technically need to
perform this since we are just making a strong branch stronger, and we
already know that it will be in the source-set of any cut involving
higher values of $\lambda$.  Note also that there may be splits as
we push the new excess up the tree.  Fortunately, [[weakPush]] handles
all aspects of this operation: pushing excess, splitting branches, and
queuing newly strong branch roots.
<<Solver public declarations>>=
    void setNextCapacityParameter(double oldLambda, double newLambda);
<<Solver method implementations>>=
void PhaseSolver::setNextCapacityParameter(double oldLambda, double newLambda)
{
    assert(newLambda >= oldLambda);
    <<remove strong nodes from graph>>
    <<increase capacity on arcs out of source>>
    <<decrease capacity on arcs into sink>>
}
@ %def setNextCapacityParameter
We increase the capacity of the arcs out of the source node.  This will
create excess at the source-adjacent node.  If this node is already
marked as strong, we can just leave the excess because we have already removed
the node and its branch from the graph.  Otherwise, we push the flow to
the (weak) root which may cause the branch to become strong.
<<increase capacity on arcs out of source>>=
    for(sourceNode->resetIterations();
        sourceNode->hasMoreNeighbors();
	sourceNode->advanceNeighbors())
    {
	Edge& edge = sourceNode->getCurrentNeighbor();
	Node& sourceAdjNode = *edge.getDest();
	if (&sourceAdjNode != sourceNode) {
	    FlowAmount delta = 0;
	    Boolean crossed = edge.increaseLambda(oldLambda, newLambda, delta);
	    assert(crossed == FALSE);
	    if (delta > 0) {
		edge.increaseFlow(delta);
		sourceAdjNode.incrementExcess(delta);
		if (sourceAdjNode.getLabel() < numNodes) {
		    weakPush(sourceAdjNode);
		}
	    }
	}
    }
@ 
Processing the arcs into the sink is more complicated because the capacity
is actually negative and increasing towards zero (but we take the absolute
value so it looks like it's decreasing).  If it crosses zero, we must move
the arc to be adjacent to the source.  change in capacity.  
<<decrease capacity on arcs into sink>>=
    for(sinkNode->resetIterations();
        sinkNode->hasMoreNeighbors();
	sinkNode->advanceNeighbors())
    {
	Edge& edge = sinkNode->getCurrentNeighbor();
	Node& sinkAdjNode = *edge.getSource();
	if (&sinkAdjNode != sinkNode) {
	    FlowAmount delta = 0;
	    Boolean crossed = edge.increaseLambda(oldLambda, newLambda, delta);
	    if (delta > 0) {
		if (crossed) {
		    <<move arc to source>>
		} else {
		    edge.decreaseFlow(delta);
		}
		sinkAdjNode.incrementExcess(delta);
		if (sinkAdjNode.getLabel() < numNodes) {
		    weakPush(sinkAdjNode);
		}
	    }
	}
    }
@ When the capacity of a sink-adjacent arc crosses zero, we need to move
it from the sink to the source.  Note that we do not need to update the
neighbor list of the node because we already removed sink as a neighbor
during initialization ([saturateSourceSinkArcs]).
<<move arc to source>>=
    sinkNode->removeNeighbor(edge);
    edge.reInit(*sourceNode, sinkAdjNode);
    sourceNode->addNeighbor(edge);
@ Next we need to set the flow amount on the arc.  This is the new,
positive capacity, which is less than delta - i.e. saturate the arc.
However, we increase the excess of the node by the full delta amount (see
above).
<<move arc to source>>=
    edge.saturate();
@
Should this be a separate function so that we can account for its
time and be able to easily remove it for comparisons?

First, we mark all of the active, strong nodes as being strong.
We do this by iterating over the buckets and then proceeding down
each branch that we find.
<<remove strong nodes from graph>>=
    int maxFullBucket = 0;
    for (int i = 1; i <= highestLabel; i++) {
        NodePtr node = buckets[i].getHead();
	while (node != nil) {
	    maxFullBucket = i;
	    labelSubtree(*node, numNodes);
	    node = node->getNext();
	}
    }
@ Next, we need to `remove' the strong nodes from the graph.
This can effectively be done by just emptying the buckets that contain
all of the strong branches.
<<remove strong nodes from graph>>=
    // remove all strong roots from bucket lists
    for (int i = 1; i <= maxFullBucket; i++) {
	buckets[i].emptyBucket();
    }
@ For maximum efficiency, we should remove all arcs from weak nodes to
strong nodes.  It's not clear if this is mandatory.  On the one hand,
one could argue that if we leave them, we're just scaning useless arcs
when a weak node later becomes strong, which is inefficient, but benign.
On the other hand, if we don't remove them, then the labels of the
strong nodes that we have removed from the graph will not increase and
might be mistaken for weak nodes based on their low labels.  This might
be OK, but it would be `resurrecting' old strong nodes, which is a waste
at the very least.
<<remove strong nodes from graph>>=
    /*
    for (int i = 1; i <= numNodes; i++) {
	Node& node = nodes[i];
	if (node.flagIsSet(STRONG_NODE) || &node == sourceNode || &node == sinkNode) {
	    continue;
	}
	for (node.resetIterations(); 
	     node.hasMoreNeighbors();
	     node.advanceNeighbors())
	{
	    Edge& edge = node.getCurrentNeighbor();
	    Node& otherNode = *edge.getOtherNode(&node);
	    if (otherNode.flagIsSet(STRONG_NODE)) {
		assert(edge.residCapacity(otherNode) == 0);
		node.removeNeighbor(edge);
	    }
	}
    }
    */

<<Solver methods>>=
    void labelSubtree(Node& node, NodeLabel l);
<<Solver method implementations>>=
void PhaseSolver::labelSubtree(Node& node, NodeLabel l)
{
    node.resetIterations();
    while (node.hasMoreChildren()) {
	Node& child = node.getCurrentChild();
	labelSubtree(child, l);
	node.advanceChildren();
    }

    setLabel(node, l);
}
@ %def labelSubtree

\subsection{Input Output Functions}
We need routines to read Dimacs problem files and write 
Dimacs flow files.  At the moment, these are member
functions on the solver.  An alternative design would
be to put them in a separate class or file to allow us
to support multiple input file formats for the same 
solver class.

Our function to read a problem instance is given a file name and
return true if it sucessfully read the problem instance from the
specified file.
<<Solver methods>>=
    Boolean readDimacsInstance(const char* filename);
@
Our code will be divided into two steps. In the first
steps we will determine the basic dimensions of the problem, initialize
the edges, and compute the degree of each node.  Once we know the degree
of the nodes, we can initialize them and add all of their neighbors.

The code below uses C standard I/O ([[stdio]]) because I'm too lazy
to figure out how to use C++ I/O streams for input.  
<<Solver method implementations>>=
Boolean PhaseSolver::readDimacsInstance(const char* filename)
{
    FILE* fp = fopen(filename, "r");
    if (fp != NULL) {
	instanceFilename = filename;
	<<local variables for reading>>
	<<read instance and initialize edges>>
	<<initialize nodes>>
    } else {
	perror("Unable to read problem instance");
	return FALSE;
    }
    return TRUE;
}
<<implementation header files>>=
#include <stdio.h>
<<Solver data>>=
    const char* instanceFilename;
<<default Solver constructor>>=
    instanceFilename = "<unknown instance>";
@ %def readDimacsInstance
Reading the problem instance is farily straightforward.  We
read lines until the end of file.  Each line
in the file is identified by a `type character' that specifies what
type of line we are reading.  The type then determines how much more
information is on the line.  The additional information will begin 
at the third character in the line.
<<read instance and initialize edges>>=
    while (!feof(fp)) {
	char buffer[500];
	if (fgets(buffer, 500, fp) == NULL) {
	    break;
	}
	char* moreInfo = &buffer[2];
	char type;
	sscanf(buffer, "%c", &type);
	<<parse line>>
    }
@ We should check that the number of edges that are supposed to be in the
file is how many we actually read.  This can be a problem if we run out of
disk space when generating an instance.
<<read instance and initialize edges>>=
    if (nextEdge < numEdges) {
	cerr << "Not enough edges - expected " << numEdges << " found " 
	     << nextEdge << endl;
	return FALSE;
    }
@ 
Parsing the type code is a pretty simple switch statment.
<<parse line>>=
    switch (type) {
	case 'c':	// comment
	    break;
	case 'p':	// problem dimensions
	    <<allocate problem instance>>
	    break;
	case 'n':   {	// specify source or sink
	    <<specify source/sink>>
	    break;
	}
	case 'a':   {	// read an edge
	    <<read edge>>
	    break;
	}
	default:
	    cerr << "Unrecognized input line: " << buffer << endl;
	    break;
    }
@
The problem instance has a problem type string, number of nodes,
and number of instances.  In addition to allocating the array
of nodes, we also allocate and zero an array of integers that we
will use to count the degree of each node as we read each edge.
Note that node id's begin at one rather than zero, so we need to
allocate an extra node (node zero), that never really gets used.
<<allocate problem instance>>=
    char typeBuffer[20];
    numTokens = sscanf(moreInfo, "%s %d %d", typeBuffer, &numNodes, &numEdges);
    if ((numTokens != 3) || (numNodes <= 0) || (numEdges <= 0)) {
	cerr << "Invalid problem instance line: " << buffer << endl;
	return FALSE;
    }
    nodes = new Node[numNodes + 1];
    edges = new Edge[numEdges];
    buckets = new NodeBucket[numNodes + 1];
    nodeDegrees = new int[numNodes + 1];
    labelCount = new int[numNodes + 1];
    nodeQ = new NodePtr[numNodes + 1];

    for (int i = 0; i <= numNodes; i++) {
	nodeDegrees[i] = labelCount[i] = 0;
	nodeQ[i] = nil;
    }
<<local variables for reading>>=
    int numTokens = 0;
    int* nodeDegrees = nil;
@ 
The source and sink in the graph are simply specified by a node number and `s'
for the source and `t' for the sink.
<<specify source/sink>>=
    char sourceSinkFlag;
    int nodeNumber;
    numTokens =  sscanf(moreInfo, "%d %c", &nodeNumber, &sourceSinkFlag);
    if ((numTokens != 2) || (nodeNumber <= 0) || (nodeNumber > numNodes)) {
	cerr << "Invalid source/sink line: " << buffer << endl;
	return FALSE;
    }

    if (sourceSinkFlag == 's') {
	sourceNode = &nodes[nodeNumber];
    } else if (sourceSinkFlag == 't') {
	sinkNode = &nodes[nodeNumber];
    } else {
	cerr << "Invalid source/sink line: " << buffer << endl;
	return FALSE;
    }
@ 
An edge is specified by a source, destination and a capacity.
However, when we are performing parametric sensitivity analysis,
we have an additional parameter, $b$, and the capacity is
actually interpreted as the $a$ parameter.
<<read edge>>=
    int source, dest, capacity;
    double bParam = 0;
    numTokens =  sscanf(moreInfo, "%d %d %d %lf", 
			&source, &dest, &capacity, &bParam);
    if ((numTokens < 3) || (source <= 0) || (source > numNodes) ||
	(dest <= 0) || (dest > numNodes)) {
	cerr << "Invalid edge line: " << buffer << endl;
	return FALSE;
    }

    if (nextEdge < numEdges) {
	if (numTokens == 3) {
	    // non-parametric must have non-negative capacity
	    if (capacity < 0) {
		cerr << "Invalid edge line: " << buffer << endl;
		return FALSE;
	    }
	    edges[nextEdge].init(nodes[source], nodes[dest], capacity);
	} else {
	    edges[nextEdge].init(nodes[source], nodes[dest], capacity, bParam);
	}
	nextEdge++;
	nodeDegrees[source]++;
	nodeDegrees[dest]++;
    } else {
	cerr << "Too many edges - graph should only contain " << numEdges << endl;
	return FALSE;
    }
<<local variables for reading>>=
    int nextEdge = 0;
@
To initialize the nodes, we call their [[init]] method to allocate
space for neighbors based on the degree of each node that we observered
while reading edges.  We can trivially assign each node it's id number.
We special-case the source and sink nodes to give them larger adjacency
lists to support parameteric analysis where nodes can move between the
source and sink depending on the value of lambda.
<<initialize nodes>>=
    // until the nodes are initialized, the nodes pointed at by sourceNode
    // and sinkNode don't have ids, so we have to compute the ids by
    // pointer arithmetic
    int sourceId = sourceNode - nodes;
    int sinkId = sinkNode - nodes;
    for (int i = 1; i <= numNodes; i++) {
	if ((i == sourceId) || (i == sinkId)) {
	nodes[i].init(i, nodeDegrees[sourceId] + nodeDegrees[sinkId]);
    } else {
	nodes[i].init(i, nodeDegrees[i]);
    }
    }
@
Once the nodes are initialized, we can easily iterate over the list of 
edges and add each endpoint to the other's list of neighbors.  Note,
we only scan the edges that we actually read (based on [[nextEdge]])
rather than the number we could have seen (specified by [[numEdges]]).
<<initialize nodes>>=
    for (int i = 0; i < nextEdge; i++) {
	edges[i].getHead()->appendNeighbor(edges[i]);
	edges[i].getTail()->appendNeighbor(edges[i]);
    }
@
Finally, let's remember to free up the memory we allocated to count
node degrees.
<<initialize nodes>>=
    delete[] nodeDegrees;
@
After we read an instance and solve it, we need to write the result
out.  The Dimacs flow file is very similar to the instance in
that every line is identified by an initial type character followed
by arguments.  
<<Solver methods>>=
    void writeDimacsFlow(ostream& dout);
<<Solver method implementations>>=
void PhaseSolver::writeDimacsFlow(ostream& dout)
{
    for (int i = 0; i < numEdges; i++) {
	edges[i].writeFlow(dout) << endl;
    }
}
@ %def writeDimacsFlow
<<header include files>>=
    class ostream;
@
We collect various statistics while the program is running.
This method prints them into the Dimacs output file as comments.
<<Solver methods>>=
    virtual void writeStats(ostream& dout);
<<Solver method implementations>>=
void PhaseSolver::writeStats(ostream& dout)
{
    <<write statistics>>
    <<write flow amount>>
}
@ %def writeStats
<<write statistics>>=
    dout << "c  numNodes:      " << numNodes        << endl;
    dout << "c  numArcs:       " << numEdges        << endl;
    dout << "c  numMergers:    " << numMergers      << endl;
    dout << "c  numPushToParent: " << numPushToParent      << endl;
    dout << "c  numSplits:     " << numSplits       << endl;
    dout << "c  numRelabels:   " << numRelabels     << endl;
    dout << "c  numLabelSkips: " << numLabelSkips   << endl;
    dout << "c  numGlobalRelabels:   " << numGlobalRelabels     << endl;
    dout << "c  numEmptyBranchScans: " << numEmptyBranchScans << endl;
    dout << "c  numRemovedNodes: " << numRemovedNodes << endl;
    dout << "c  lowestLabel:   " << lowestLabel     << endl;
<<Solver protected members>>=
    int numMergers;
    int numPushToParent;
    int numSplits;
    int numRelabels;
    int numLabelSkips;
    int numGlobalRelabels;
    int numEmptyBranchScans;
    int numRemovedNodes;
@ %def numMergers numPushToParent numSplits numRelabels numLabelSkips numGlobalRelabels numEmptyBranchScans numRemovedNodes
<<default Solver constructor>>=
    numMergers = numPushToParent = numSplits = numRelabels = numLabelSkips = 0;
    numGlobalRelabels = numEmptyBranchScans = numRemovedNodes = 0;
@
To compute the flow amount, we just scan the neighbors of the source
node and sum the flows on those arcs.  If we were paranoid, we'd also
scan the sink arcs and compare the amounts.  We write it as a comment as
well as an 's' line.
<<write flow amount>>=
    int flowAmt = 0;
    for (sourceNode->resetIterations(); 
	 sourceNode->hasMoreNeighbors();
	 sourceNode->advanceNeighbors())
    {
	Edge& edge = sourceNode->getCurrentNeighbor();
	flowAmt += edge.getFlow();
    }
    dout << "c  flowValue:   " << flowAmt     << endl;
    dout << "c"                                     << endl;
    dout << "s " << flowAmt << endl;
@
We also use the methods of this class for data file conversion when
converting from parametric files to non-parameteric files.  We used to
do this with a separate program written in Python, but the floating point
semantics are slightly different.  Therefore, we can read a parameteric
instance and set a value for lambda, then dump it out.  This should have
identical values for the arc capacities as when we use the parametric solver.
<<Solver methods>>=
    void writeInstance(ostream& dout);
<<Solver method implementations>>=
void PhaseSolver::writeInstance(ostream& dout)
{
    dout << "p  max  " << numNodes << "  " << numEdges << endl;
    dout << "n  " << sourceNode->getId() << " s" << endl;
    dout << "n  " << sinkNode->getId() << " t" << endl;
    for (int i = 0; i < numEdges; i++) {
	edges[i].writeArcInstance(dout) << endl;
    }
}
@ %def writeDimacsFlow
@
Here is a method to print the final disposition of all of the nodes
in the graph.  This should be called before converting the pseudoflow
to a flow.
<<Solver methods>>=
    void dumpNodes(ostream& out);
<<Solver method implementations>>=
void PhaseSolver::dumpNodes(ostream& out)
{
    out << "c" << endl
	<< "c  Node Dump" << endl
        << "c Node  id   excess  parent   label  strong/weak" << endl;

    for (int i = 1; i <= numNodes; i++) {
	Node& node = nodes[i];
	out << "c " << setw(8) << node.getId() << " " << setw(8);
	if (&node == sourceNode) {
	    out << "source";
	} else if (&node == sinkNode) {
	    out << "sink";
	} else {
	     out << node.getExcess() << setw(8);
	     if (node.isRootNode()) {
		out << "NULL";
	     } else {
		out << node.getParentNode()->getId();
	     }

	     out << setw(8) << node.getLabel();
	     out << setw(8) <<
		 (node.isStrong(FALSE) ? "strong" : "weak");
	 }
	 out << endl;
    }
    out << endl << "c" << endl;
}
@ %def dumpNodes
\subsection{Debug Code}
Here is a debug method to find all branches (weak and strong) and
check them.
<<Solver methods>>=
    void checkBranches();
<<Solver method implementations>>=
void PhaseSolver::checkBranches()
{
    for (int i = 0; i < numNodes; i++) {
	Node& node = nodes[i];
	if ((&node == sourceNode) || (&node == sinkNode)) {
	    continue;
	}
	if (node.isRootNode()) {
	    if (node.checkTree(nil) == FALSE) {
		cout << endl << "Quitting after one bad branch" << endl;
		break;
	    }
	}
    }
}
@ %def checkBranches
Here is a method to check for mergers between strong nodes and weak nodes.
After we complete solving, there should be no mergers available.
<<Solver methods>>=
    void checkMergers();
<<Solver method implementations>>=
void PhaseSolver::checkMergers()
{
    cout << "Checking for missed mergers." << endl;
    for (int i = 0; i < numNodes; i++) {
	Node& node = nodes[i];
	if ((&node == sourceNode) || (&node == sinkNode)) {
	    continue;
	}
	if (node.isStrong(FALSE)) {
	    ElIterator it = node.getNeighbors();
	    for (EdgePtr edge = it.getNext(); edge != nil; edge=it.getNext()) {
		Node& neighbor = *edge->getOtherNode(&node);
		if ((neighbor.isStrong(FALSE) == FALSE) && 
		    (edge->residCapacity(node) > 0))
		{
		    TRACE(trout << "Found merger from " 
		         << node.getId() << " (" << node.getLabel() << ")  to "
		         << neighbor.getId() << " (" << neighbor.getLabel() << ")"
			 << endl);
		}
	    }
	}
    }
}
@ %def checkMergers
\subsection{File Boiler Plate}
<<C++ overhead>>=
    PhaseSolver();
<<Solver method implementations>>=
PhaseSolver::PhaseSolver()
{
    <<default Solver constructor>>
}
@
We start with the boiler-plate implementation file. 
<<*>>=
#include "PhaseSolver.h"
#include "debug.h"
#include <iostream.h>
#include <fstream.h>
#include <iomanip.h>
#include <assert.h>
<<implementation header files>>

<<Solver method implementations>>
@
Through magic of the C preprocessor and the macro-like facilities
of {\em noweb}, we can easily define the inline functions
out-of-line to allow us to collect better profile information - 
i.e. collect data on the inline functions that would otherwise
not show up in the function call traces of the profiler.
<<*>>=
#ifndef INLINE_SOLVER
#define INLINE /*inline*/
<<Solver inline implementations>>
#undef INLINE 
#endif /*INLINE_SOLVER*/
@
Now, we'll add the simplex solver methods.
<<*>>=
    <<Simplex implementations>>
@
The header defines the [[PhaseSolver]] class, its member functions, member
data, and inline functions.  Again, we have more boiler-plate.
<<header>>=
#ifndef SOLVER_H
#define SOLVER_H
#include "types.h"
#include "Node.h"
<<header include files>>
@ We have used some pointer to member function types to select run-time
behavior.
<<header>>=
class PhaseSolver;
typedef void (PhaseSolver::* AddBranchPtr)(Node& root);
typedef void (PhaseSolver::* RenormalizePtr)(Node& strongRoot, Node& weakNode);
@ %def AddBranchPtr RenormalizePtr
Then, we need to define the node buckets before we use them.
<<header>>=
    <<Node Bucket definition>>
@ 
Now, we can finally get to defining the solver.
<<header>>=
class PhaseSolver 
{
public:
    <<Solver public declarations>>
    <<Solver methods>>
    <<C++ overhead>>
    <<public Solver data>>
protected:
    <<Solver protected members>>
private:
    <<Solver data>>
    <<Solver private members>>
};
@ %def PhaseSolver
In case we want the inline functions to really be inlined, we
also define them here in the header file.
<<header>>=
#ifdef INLINE_SOLVER
#define INLINE inline
<<Solver inline implementations>>
#undef INLINE 
#endif /*INLINE_SOLVER*/
#endif /*SOLVER_H*/
@
And finally, let's fill out the definition for Node Buckets.
<<Node Bucket definition>>=
class NodeBucket
{
    public: NodeBucket() { head = tail = nil; }
    <<Node Bucket methods>>
    <<Node Bucket data>>
};
<<Node Bucket inline implementations>>
@
We can also add the simplex solver class.
<<header>>=
    <<Simplex declaration>>

